<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>transformers.tokenization_utils_base &mdash; RecBot v0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            RecBot
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">QUICK_START</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/installation.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">USAGE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../usage/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage/examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage/main_usage.html">Main Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usage/interface.html">Interactive Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RESOURCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../resource/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resource/module_zoo.html">Module Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resource/model_zoo.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resource/data_zoo.html">Dataset Zoo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../development/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../development/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../development/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../development/others.html">Others</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/utils.html">recbot.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/models.html">recbot.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/modules.html">recbot.modules</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">RecBot</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">transformers.tokenization_utils_base</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for transformers.tokenization_utils_base</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1"># Copyright 2020 The HuggingFace Inc. team.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Base classes common to both the slow and the fast tokenization classes: PreTrainedTokenizerBase (host all the user</span>
<span class="sd">fronting encoding methods) Special token mixing (host the special tokens logic) and BatchEncoding (wrap the dictionary</span>
<span class="sd">of output with special method for the Fast tokenizers)</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span><span class="p">,</span> <span class="n">UserDict</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Mapping</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">__version__</span>
<span class="kn">from</span> <span class="nn">.dynamic_module_utils</span> <span class="kn">import</span> <span class="n">custom_object_save</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ExplicitEnum</span><span class="p">,</span>
    <span class="n">PaddingStrategy</span><span class="p">,</span>
    <span class="n">PushToHubMixin</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
    <span class="n">add_end_docstrings</span><span class="p">,</span>
    <span class="n">cached_file</span><span class="p">,</span>
    <span class="n">copy_func</span><span class="p">,</span>
    <span class="n">download_url</span><span class="p">,</span>
    <span class="n">extract_commit_hash</span><span class="p">,</span>
    <span class="n">is_flax_available</span><span class="p">,</span>
    <span class="n">is_jax_tensor</span><span class="p">,</span>
    <span class="n">is_numpy_array</span><span class="p">,</span>
    <span class="n">is_offline_mode</span><span class="p">,</span>
    <span class="n">is_remote_url</span><span class="p">,</span>
    <span class="n">is_tf_available</span><span class="p">,</span>
    <span class="n">is_tf_tensor</span><span class="p">,</span>
    <span class="n">is_tokenizers_available</span><span class="p">,</span>
    <span class="n">is_torch_available</span><span class="p">,</span>
    <span class="n">is_torch_device</span><span class="p">,</span>
    <span class="n">is_torch_tensor</span><span class="p">,</span>
    <span class="n">logging</span><span class="p">,</span>
    <span class="n">to_py_obj</span><span class="p">,</span>
    <span class="n">torch_required</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">is_torch_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">torch</span>
    <span class="k">if</span> <span class="n">is_tf_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
    <span class="k">if</span> <span class="n">is_flax_available</span><span class="p">():</span>
        <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># noqa: F401</span>


<span class="k">if</span> <span class="n">is_tokenizers_available</span><span class="p">():</span>
    <span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">AddedToken</span>
    <span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Encoding</span> <span class="k">as</span> <span class="n">EncodingFast</span>
<span class="k">else</span><span class="p">:</span>

    <span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eq</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">class</span> <span class="nc">AddedToken</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        AddedToken represents a token to be added to a Tokenizer An AddedToken can have special options defining the</span>
<span class="sd">        way it should behave.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">content</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
        <span class="n">single_word</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">lstrip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">rstrip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span>

    <span class="nd">@dataclass</span>
    <span class="k">class</span> <span class="nc">EncodingFast</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This is dummy class because without the `tokenizers` library we don&#39;t have these objects anyway&quot;&quot;&quot;</span>

        <span class="k">pass</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="n">VERY_LARGE_INTEGER</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e30</span><span class="p">)</span>  <span class="c1"># This is used to set the max input length for a model with infinite size input</span>
<span class="n">LARGE_INTEGER</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e20</span><span class="p">)</span>  <span class="c1"># This is used when we need something big but slightly smaller than VERY_LARGE_INTEGER</span>

<span class="c1"># Define type aliases and NamedTuples</span>
<span class="n">TextInput</span> <span class="o">=</span> <span class="nb">str</span>
<span class="n">PreTokenizedInput</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="n">EncodedInput</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="n">TextInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
<span class="n">PreTokenizedInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
<span class="n">EncodedInputPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span>


<span class="c1"># Slow tokenizers used to be saved in three separated files</span>
<span class="n">SPECIAL_TOKENS_MAP_FILE</span> <span class="o">=</span> <span class="s2">&quot;special_tokens_map.json&quot;</span>
<span class="n">ADDED_TOKENS_FILE</span> <span class="o">=</span> <span class="s2">&quot;added_tokens.json&quot;</span>
<span class="n">TOKENIZER_CONFIG_FILE</span> <span class="o">=</span> <span class="s2">&quot;tokenizer_config.json&quot;</span>

<span class="c1"># Fast tokenizers (provided by HuggingFace tokenizer&#39;s library) can be saved in a single file</span>
<span class="n">FULL_TOKENIZER_FILE</span> <span class="o">=</span> <span class="s2">&quot;tokenizer.json&quot;</span>
<span class="n">_re_tokenizer_file</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;tokenizer\.(.*)\.json&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TruncationStrategy</span><span class="p">(</span><span class="n">ExplicitEnum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Possible values for the `truncation` argument in [`PreTrainedTokenizerBase.__call__`]. Useful for tab-completion in</span>
<span class="sd">    an IDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ONLY_FIRST</span> <span class="o">=</span> <span class="s2">&quot;only_first&quot;</span>
    <span class="n">ONLY_SECOND</span> <span class="o">=</span> <span class="s2">&quot;only_second&quot;</span>
    <span class="n">LONGEST_FIRST</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span>
    <span class="n">DO_NOT_TRUNCATE</span> <span class="o">=</span> <span class="s2">&quot;do_not_truncate&quot;</span>


<span class="k">class</span> <span class="nc">CharSpan</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Character span in the original string.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (`int`): Index of the first character in the original string.</span>
<span class="sd">        end (`int`): Index of the character following the last character in the original string.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">start</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">class</span> <span class="nc">TokenSpan</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Token span in an encoded string (list of tokens).</span>

<span class="sd">    Args:</span>
<span class="sd">        start (`int`): Index of the first token in the span.</span>
<span class="sd">        end (`int`): Index of the token following the last token in the span.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">start</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">end</span><span class="p">:</span> <span class="nb">int</span>


<span class="k">class</span> <span class="nc">BatchEncoding</span><span class="p">(</span><span class="n">UserDict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Holds the output of the [`~tokenization_utils_base.PreTrainedTokenizerBase.__call__`],</span>
<span class="sd">    [`~tokenization_utils_base.PreTrainedTokenizerBase.encode_plus`] and</span>
<span class="sd">    [`~tokenization_utils_base.PreTrainedTokenizerBase.batch_encode_plus`] methods (tokens, attention_masks, etc).</span>

<span class="sd">    This class is derived from a python dictionary and can be used as a dictionary. In addition, this class exposes</span>
<span class="sd">    utility methods to map from word/character space to token space.</span>

<span class="sd">    Args:</span>
<span class="sd">        data (`dict`):</span>
<span class="sd">            Dictionary of lists/arrays/tensors returned by the `__call__`/`encode_plus`/`batch_encode_plus` methods</span>
<span class="sd">            (&#39;input_ids&#39;, &#39;attention_mask&#39;, etc.).</span>
<span class="sd">        encoding (`tokenizers.Encoding` or `Sequence[tokenizers.Encoding]`, *optional*):</span>
<span class="sd">            If the tokenizer is a fast tokenizer which outputs additional information like mapping from word/character</span>
<span class="sd">            space to token space the `tokenizers.Encoding` instance or list of instance (for batches) hold this</span>
<span class="sd">            information.</span>
<span class="sd">        tensor_type (`Union[None, str, TensorType]`, *optional*):</span>
<span class="sd">            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at</span>
<span class="sd">            initialization.</span>
<span class="sd">        prepend_batch_axis (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">            Whether or not to add a batch axis when converting to tensors (see `tensor_type` above).</span>
<span class="sd">        n_sequences (`Optional[int]`, *optional*):</span>
<span class="sd">            You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at</span>
<span class="sd">            initialization.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">encoding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tensor_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_sequences</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">EncodingFast</span><span class="p">):</span>
            <span class="n">encoding</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoding</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="o">=</span> <span class="n">encoding</span>

        <span class="k">if</span> <span class="n">n_sequences</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">encoding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">):</span>
            <span class="n">n_sequences</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">n_sequences</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_n_sequences</span> <span class="o">=</span> <span class="n">n_sequences</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convert_to_tensors</span><span class="p">(</span><span class="n">tensor_type</span><span class="o">=</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">n_sequences</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: The number of sequences used to generate each sample from the batch encoded in this</span>
<span class="sd">        [`BatchEncoding`]. Currently can be one of `None` (unknown), `1` (a single sentence) or `2` (a pair of</span>
<span class="sd">        sentences)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_sequences</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_fast</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `bool`: Indicate whether this [`BatchEncoding`] was generated from the result of a [`PreTrainedTokenizerFast`]</span>
<span class="sd">        or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">EncodingFast</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If the key is a string, returns the value of the dict associated to `key` (&#39;input_ids&#39;, &#39;attention_mask&#39;,</span>
<span class="sd">        etc.).</span>

<span class="sd">        If the key is an integer, get the `tokenizers.Encoding` for batch item with index `key`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="s2">&quot;Indexing with integers (to access backend Encoding for a given batch index) &quot;</span>
                <span class="s2">&quot;is not available when using Python based tokenizers&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s2">&quot;encodings&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;data&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="s2">&quot;encodings&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;encodings&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>

    <span class="c1"># After this point:</span>
    <span class="c1"># Extended properties and methods only available for fast (Rust-based) tokenizers</span>
    <span class="c1"># provided by HuggingFace tokenizers library.</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encodings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">EncodingFast</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[List[tokenizers.Encoding]]`: The list all encodings from the tokenization process. Returns `None` if</span>
<span class="sd">        the input was tokenized through Python (i.e., not a fast) tokenizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span>

    <span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the list of tokens (sub-parts of the input strings after word/subword splitting and before conversion to</span>
<span class="sd">        integer indices) at a given batch index (only works for the output of a fast tokenizer).</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (`int`, *optional*, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[str]`: The list of tokens at that index.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;tokens() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`&quot;</span>
                <span class="s2">&quot; class).&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">sequence_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to the id of their original sentences:</span>

<span class="sd">            - `None` for special tokens added around or between sequences,</span>
<span class="sd">            - `0` for tokens corresponding to words in the first sequence,</span>
<span class="sd">            - `1` for tokens corresponding to words in the second sequence when a pair of sequences was jointly</span>
<span class="sd">              encoded.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (`int`, *optional*, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[Optional[int]]`: A list indicating the sequence id corresponding to each token. Special tokens added</span>
<span class="sd">            by the tokenizer are mapped to `None` and other tokens are mapped to the index of their corresponding</span>
<span class="sd">            sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;sequence_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`&quot;</span>
                <span class="s2">&quot; class).&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">sequence_ids</span>

    <span class="k">def</span> <span class="nf">words</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (`int`, *optional*, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by the</span>
<span class="sd">            tokenizer are mapped to `None` and other tokens are mapped to the index of their corresponding word</span>
<span class="sd">            (several tokens will be mapped to the same word index if they are parts of that word).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;words() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`&quot;</span>
                <span class="s2">&quot; class).&quot;</span>
            <span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`BatchEncoding.words()` property is deprecated and should be replaced with the identical, &quot;</span>
            <span class="s2">&quot;but more self-explanatory `BatchEncoding.word_ids()` property.&quot;</span><span class="p">,</span>
            <span class="ne">FutureWarning</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">word_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_index (`int`, *optional*, defaults to 0): The index to access in the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[Optional[int]]`: A list indicating the word corresponding to each token. Special tokens added by the</span>
<span class="sd">            tokenizer are mapped to `None` and other tokens are mapped to the index of their corresponding word</span>
<span class="sd">            (several tokens will be mapped to the same word index if they are parts of that word).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`&quot;</span>
                <span class="s2">&quot; class).&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_ids</span>

    <span class="k">def</span> <span class="nf">token_to_sequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the sequence represented by the given token. In the general use case, this method returns `0`</span>
<span class="sd">        for a single sequence or the first sequence of a pair, and `1` for the second sequence of a pair</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.token_to_sequence(token_index)` if batch size is 1</span>
<span class="sd">        - `self.token_to_sequence(batch_index, token_index)` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,</span>
<span class="sd">        words are defined by the user). In this case it allows to easily associate encoded tokens with provided</span>
<span class="sd">        tokenized words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the token in the</span>
<span class="sd">                sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `int`: Index of the word in the input sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_sequence() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">token_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_sequence</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">token_to_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the word corresponding (i.e. comprising) to an encoded token in a sequence of the batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.token_to_word(token_index)` if batch size is 1</span>
<span class="sd">        - `self.token_to_word(batch_index, token_index)` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e.,</span>
<span class="sd">        words are defined by the user). In this case it allows to easily associate encoded tokens with provided</span>
<span class="sd">        tokenized words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the token in the</span>
<span class="sd">                sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `int`: Index of the word in the input sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_word() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">token_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_word</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">word_to_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_word_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TokenSpan</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the encoded token span corresponding to a word in a sequence of the batch.</span>

<span class="sd">        Token spans are returned as a [`~tokenization_utils_base.TokenSpan`] with:</span>

<span class="sd">        - **start** -- Index of the first token.</span>
<span class="sd">        - **end** -- Index of the token following the last token.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.word_to_tokens(word_index, sequence_index: int = 0)` if batch size is 1</span>
<span class="sd">        - `self.word_to_tokens(batch_index, word_index, sequence_index: int = 0)` if batch size is greater or equal to</span>
<span class="sd">          1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_word_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprises one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence.</span>
<span class="sd">            word_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (`int`, *optional*, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided word index belongs to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Optional [`~tokenization_utils_base.TokenSpan`] Span of tokens in the encoded sequence. Returns `None` if</span>
<span class="sd">            no tokens correspond to the word.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;word_to_tokens() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">if</span> <span class="n">batch_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">+</span> <span class="n">batch_index</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_seq_len</span> <span class="o">+</span> <span class="n">word_index</span>
        <span class="n">span</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_to_tokens</span><span class="p">(</span><span class="n">word_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">TokenSpan</span><span class="p">(</span><span class="o">*</span><span class="n">span</span><span class="p">)</span> <span class="k">if</span> <span class="n">span</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">token_to_chars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_token_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">token_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CharSpan</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the character span corresponding to an encoded token in a sequence of the batch.</span>

<span class="sd">        Character spans are returned as a [`~tokenization_utils_base.CharSpan`] with:</span>

<span class="sd">        - **start** -- Index of the first character in the original string associated to the token.</span>
<span class="sd">        - **end** -- Index of the character following the last character in the original string associated to the</span>
<span class="sd">          token.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.token_to_chars(token_index)` if batch size is 1</span>
<span class="sd">        - `self.token_to_chars(batch_index, token_index)` if batch size is greater or equal to 1</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_token_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the token in the sequence.</span>
<span class="sd">            token_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the token or tokens in</span>
<span class="sd">                the sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`~tokenization_utils_base.CharSpan`]: Span of characters in the original string, or None, if the token</span>
<span class="sd">            (e.g. &lt;s&gt;, &lt;/s&gt;) doesn&#39;t correspond to any chars in the origin string.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;token_to_chars() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">token_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">token_index</span> <span class="o">=</span> <span class="n">batch_or_token_index</span>
        <span class="n">span_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">token_to_chars</span><span class="p">(</span><span class="n">token_index</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">CharSpan</span><span class="p">(</span><span class="o">*</span><span class="n">span_indices</span><span class="p">)</span> <span class="k">if</span> <span class="n">span_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">char_to_token</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_char_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">char_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the index of the token in the encoded output comprising a character in the original string for a sequence</span>
<span class="sd">        of the batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.char_to_token(char_index)` if batch size is 1</span>
<span class="sd">        - `self.char_to_token(batch_index, char_index)` if batch size is greater or equal to 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_char_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence</span>
<span class="sd">            char_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (`int`, *optional*, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided character index belongs to.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `int`: Index of the token.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;char_to_token() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">char_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">char_to_token</span><span class="p">(</span><span class="n">char_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">word_to_chars</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_word_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CharSpan</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the character span in the original string corresponding to given word in a sequence of the batch.</span>

<span class="sd">        Character spans are returned as a CharSpan NamedTuple with:</span>

<span class="sd">        - start: index of the first character in the original string</span>
<span class="sd">        - end: index of the character following the last character in the original string</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.word_to_chars(word_index)` if batch size is 1</span>
<span class="sd">        - `self.word_to_chars(batch_index, word_index)` if batch size is greater or equal to 1</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_word_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the word in the sequence</span>
<span class="sd">            word_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the word in the</span>
<span class="sd">                sequence.</span>
<span class="sd">            sequence_index (`int`, *optional*, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided word index belongs to.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `CharSpan` or `List[CharSpan]`: Span(s) of the associated character or characters in the string. CharSpan</span>
<span class="sd">            are NamedTuple with:</span>

<span class="sd">                - start: index of the first character associated to the token in the original string</span>
<span class="sd">                - end: index of the character following the last character associated to the token in the original</span>
<span class="sd">                  string</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;word_to_chars() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">word_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">word_index</span> <span class="o">=</span> <span class="n">batch_or_word_index</span>
        <span class="k">return</span> <span class="n">CharSpan</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">word_to_chars</span><span class="p">(</span><span class="n">word_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">char_to_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_or_char_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">char_index</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the word in the original string corresponding to a character in the original string of a sequence of the</span>
<span class="sd">        batch.</span>

<span class="sd">        Can be called as:</span>

<span class="sd">        - `self.char_to_word(char_index)` if batch size is 1</span>
<span class="sd">        - `self.char_to_word(batch_index, char_index)` if batch size is greater than 1</span>

<span class="sd">        This method is particularly suited when the input sequences are provided as pre-tokenized sequences (i.e. words</span>
<span class="sd">        are defined by the user). In this case it allows to easily associate encoded tokens with provided tokenized</span>
<span class="sd">        words.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_or_char_index (`int`):</span>
<span class="sd">                Index of the sequence in the batch. If the batch only comprise one sequence, this can be the index of</span>
<span class="sd">                the character in the original string.</span>
<span class="sd">            char_index (`int`, *optional*):</span>
<span class="sd">                If a batch index is provided in *batch_or_token_index*, this can be the index of the character in the</span>
<span class="sd">                original string.</span>
<span class="sd">            sequence_index (`int`, *optional*, defaults to 0):</span>
<span class="sd">                If pair of sequences are encoded in the batch this can be used to specify which sequence in the pair (0</span>
<span class="sd">                or 1) the provided character index belongs to.</span>


<span class="sd">        Returns:</span>
<span class="sd">            `int` or `List[int]`: Index or indices of the associated encoded token(s).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;char_to_word() is not available when using Python based tokenizers&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">char_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_index</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">char_index</span> <span class="o">=</span> <span class="n">batch_or_char_index</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encodings</span><span class="p">[</span><span class="n">batch_index</span><span class="p">]</span><span class="o">.</span><span class="n">char_to_word</span><span class="p">(</span><span class="n">char_index</span><span class="p">,</span> <span class="n">sequence_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">convert_to_tensors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensor_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert the inner content to tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            tensor_type (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="sd">                The type of tensors to use. If `str`, should be one of the values of the enum [`~utils.TensorType`]. If</span>
<span class="sd">                `None`, no modification is done.</span>
<span class="sd">            prepend_batch_axis (`int`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to add the batch dimension during the conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tensor_type</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>

        <span class="c1"># Convert to TensorType</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">):</span>
            <span class="n">tensor_type</span> <span class="o">=</span> <span class="n">TensorType</span><span class="p">(</span><span class="n">tensor_type</span><span class="p">)</span>

        <span class="c1"># Get a function reference for the correct framework</span>
        <span class="k">if</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">TENSORFLOW</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tf_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.&quot;</span>
                <span class="p">)</span>
            <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">is_tensor</span>
        <span class="k">elif</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">PYTORCH</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_torch_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Unable to convert output to PyTorch tensors format, PyTorch is not installed.&quot;</span><span class="p">)</span>
            <span class="kn">import</span> <span class="nn">torch</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span>
        <span class="k">elif</span> <span class="n">tensor_type</span> <span class="o">==</span> <span class="n">TensorType</span><span class="o">.</span><span class="n">JAX</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_flax_available</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Unable to convert output to JAX tensors format, JAX is not installed.&quot;</span><span class="p">)</span>
            <span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>  <span class="c1"># noqa: F811</span>

            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">is_jax_tensor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">as_tensor</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span>
            <span class="n">is_tensor</span> <span class="o">=</span> <span class="n">is_numpy_array</span>

        <span class="c1"># Do the tensor conversion in batch</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">prepend_batch_axis</span><span class="p">:</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
                    <span class="n">tensor</span> <span class="o">=</span> <span class="n">as_tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

                    <span class="c1"># Removing this for now in favor of controlling the shape with `prepend_batch_axis`</span>
                    <span class="c1"># # at-least2d</span>
                    <span class="c1"># if tensor.ndim &gt; 2:</span>
                    <span class="c1">#     tensor = tensor.squeeze(0)</span>
                    <span class="c1"># elif tensor.ndim &lt; 2:</span>
                    <span class="c1">#     tensor = tensor[None, :]</span>

                    <span class="bp">self</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
            <span class="k">except</span><span class="p">:</span>  <span class="c1"># noqa E722</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Unable to create tensor returning overflowing tokens of different lengths. &quot;</span>
                        <span class="s2">&quot;Please see if a fast version of this tokenizer is available to have this feature available.&quot;</span>
                    <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Unable to create tensor, you should probably activate truncation and/or padding with&quot;</span>
                    <span class="s2">&quot; &#39;padding=True&#39; &#39;truncation=True&#39; to have batched tensors with the same length. Perhaps your&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; features (`</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">` in this case) have excessive nesting (inputs type `list` where type `int` is&quot;</span>
                    <span class="s2">&quot; expected).&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@torch_required</span>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;torch.device&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;BatchEncoding&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Send all values to device by calling `v.to(device)` (PyTorch only).</span>

<span class="sd">        Args:</span>
<span class="sd">            device (`str` or `torch.device`): The device to put the tensors on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`BatchEncoding`]: The same instance after modification.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># This check catches things like APEX blindly calling &quot;to&quot; on all inputs to a module</span>
        <span class="c1"># Otherwise it passes the casts down and casts the LongTensor containing the token idxs</span>
        <span class="c1"># into a HalfTensor</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_torch_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempting to cast a BatchEncoding to type </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">. This is not supported.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>


<span class="k">class</span> <span class="nc">SpecialTokensMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A mixin derived by [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`] to handle specific behaviors related to</span>
<span class="sd">    special tokens. In particular, this class hold the attributes which can be used to directly access these special</span>
<span class="sd">    tokens in a model-independent manner and allow to set and update the special tokens.</span>

<span class="sd">    Args:</span>
<span class="sd">        bos_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token representing the beginning of a sentence.</span>
<span class="sd">        eos_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token representing the end of a sentence.</span>
<span class="sd">        unk_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token representing an out-of-vocabulary token.</span>
<span class="sd">        sep_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token separating two different sentences in the same input (used by BERT for instance).</span>
<span class="sd">        pad_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by</span>
<span class="sd">            attention mechanisms or loss computation.</span>
<span class="sd">        cls_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token representing the class of the input (used by BERT for instance).</span>
<span class="sd">        mask_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A special token representing a masked token (used by masked-language modeling pretraining objectives, like</span>
<span class="sd">            BERT).</span>
<span class="sd">        additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="sd">            A tuple or a list of additional special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">SPECIAL_TOKENS_ATTRIBUTES</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;bos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;eos_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;unk_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sep_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;pad_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cls_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;mask_token&quot;</span><span class="p">,</span>
        <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>

        <span class="c1"># We directly set the hidden value to allow initialization with special tokens</span>
        <span class="c1"># which are not yet in the vocabulary. Necessary for serialization/de-serialization</span>
        <span class="c1"># TODO clean this up at some point (probably by switching to fast tokenizers)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)),</span> <span class="sa">f</span><span class="s2">&quot;Value </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> is not a list or tuple&quot;</span>
                    <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span>
                    <span class="p">),</span> <span class="s2">&quot;One of the tokens is not a string or an AddedToken&quot;</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)):</span>
                    <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;special token </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> has to be either str or AddedToken but got: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sanitize_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Make sure that all the special tokens attributes of the tokenizer (`tokenizer.mask_token`,</span>
<span class="sd">        `tokenizer.cls_token`, etc.) are in the vocabulary.</span>

<span class="sd">        Add the missing ones to the vocabulary if needed.</span>

<span class="sd">        Return:</span>
<span class="sd">            `int`: The number of tokens added in the vocabulary during the operation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If</span>
<span class="sd">        special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the</span>
<span class="sd">        current vocabulary).</span>

<span class="sd">        Note,None When adding new tokens to the vocabulary, you should make sure to also resize the token embedding</span>
<span class="sd">        matrix of the model so that its embedding matrix matches the tokenizer.</span>

<span class="sd">        In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.</span>

<span class="sd">        Using `add_special_tokens` will ensure your special tokens can be used in several ways:</span>

<span class="sd">        - Special tokens are carefully handled by the tokenizer (they are never split).</span>
<span class="sd">        - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This</span>
<span class="sd">          makes it easy to develop model-agnostic training and fine-tuning scripts.</span>

<span class="sd">        When possible, special tokens are already registered for provided pretrained models (for instance</span>
<span class="sd">        [`BertTokenizer`] `cls_token` is already registered to be :obj*&#39;[CLS]&#39;* and XLM&#39;s one is also registered to be</span>
<span class="sd">        `&#39;&lt;/s&gt;&#39;`).</span>

<span class="sd">        Args:</span>
<span class="sd">            special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):</span>
<span class="sd">                Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,</span>
<span class="sd">                `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].</span>

<span class="sd">                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer</span>
<span class="sd">                assign the index of the `unk_token` to them).</span>

<span class="sd">        Returns:</span>
<span class="sd">            `int`: Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        # Let&#39;s see how to add a new classification token to GPT-2</span>
<span class="sd">        tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</span>
<span class="sd">        model = GPT2Model.from_pretrained(&quot;gpt2&quot;)</span>

<span class="sd">        special_tokens_dict = {&quot;cls_token&quot;: &quot;&lt;CLS&gt;&quot;}</span>

<span class="sd">        num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)</span>
<span class="sd">        print(&quot;We have added&quot;, num_added_toks, &quot;tokens&quot;)</span>
<span class="sd">        # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="sd">        model.resize_token_embeddings(len(tokenizer))</span>

<span class="sd">        assert tokenizer.cls_token == &quot;&lt;CLS&gt;&quot;</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">special_tokens_dict</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="n">added_tokens</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">assert</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is not a special token&quot;</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Assigning </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> to the </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> key of the tokenizer&quot;</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;additional_special_tokens&quot;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">value</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Tokens </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> for key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> should all be str or AddedToken instances&quot;</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">)</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Token </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2"> for key </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> should be a str or an AddedToken instance&quot;</span>
                <span class="n">added_tokens</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">([</span><span class="n">value</span><span class="p">],</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">added_tokens</span>

    <span class="k">def</span> <span class="nf">add_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]],</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to</span>
<span class="sd">        it with indices starting from length of the current vocabulary and and will be isolated before the tokenization</span>
<span class="sd">        algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore</span>
<span class="sd">        not treated in the same way.</span>

<span class="sd">        Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix</span>
<span class="sd">        of the model so that its embedding matrix matches the tokenizer.</span>

<span class="sd">        In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.</span>

<span class="sd">        Args:</span>
<span class="sd">            new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):</span>
<span class="sd">                Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string</span>
<span class="sd">                token to let you personalize its behavior: whether this token should only match against a single word,</span>
<span class="sd">                whether this token should strip all potential whitespaces on the left side, whether this token should</span>
<span class="sd">                strip all potential whitespaces on the right side, etc.</span>
<span class="sd">            special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Can be used to specify if the token is a special token. This mostly change the normalization behavior</span>
<span class="sd">                (special tokens like CLS or [MASK] are usually not lower-cased for instance).</span>

<span class="sd">                See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `int`: Number of tokens added to the vocabulary.</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        # Let&#39;s see how to increase the vocabulary of Bert model and tokenizer</span>
<span class="sd">        tokenizer = BertTokenizerFast.from_pretrained(&quot;bert-base-uncased&quot;)</span>
<span class="sd">        model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)</span>

<span class="sd">        num_added_toks = tokenizer.add_tokens([&quot;new_tok1&quot;, &quot;my_new-tok2&quot;])</span>
<span class="sd">        print(&quot;We have added&quot;, num_added_toks, &quot;tokens&quot;)</span>
<span class="sd">        # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.</span>
<span class="sd">        model.resize_token_embeddings(len(tokenizer))</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">new_tokens</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">new_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_tokens</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_tokens</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_add_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_tokens</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">]],</span> <span class="n">special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Beginning of sentence token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using bos_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: End of sentence token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using eos_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Unknown token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using unk_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not</span>
<span class="sd">        having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using sep_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Padding token. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using pad_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full</span>
<span class="sd">        depth of the model. Log an error if used while not having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using cls_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not</span>
<span class="sd">        having been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using mask_token, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been</span>
<span class="sd">        set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Using additional_special_tokens, but it is not set yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span><span class="p">]</span>

    <span class="nd">@bos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@eos_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@unk_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@sep_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@pad_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@cls_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@mask_token</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@additional_special_tokens</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not</span>
<span class="sd">        been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been</span>
<span class="sd">        set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eos_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input</span>
<span class="sd">        sequence. Returns `None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sep_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `int`: Id of the padding token type in the vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token_type_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence</span>
<span class="sd">        leveraging self-attention along the full depth of the model.</span>

<span class="sd">        Returns `None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mask_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language</span>
<span class="sd">        modeling. Returns `None` if the token has not been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_token</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having</span>
<span class="sd">        been set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_special_tokens</span><span class="p">)</span>

    <span class="nd">@bos_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bos_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@eos_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eos_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@unk_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unk_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@sep_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sep_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@pad_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pad_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@cls_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cls_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@mask_token_id</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mask_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mask_token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="nd">@additional_special_tokens_ids</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">additional_special_tokens_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_additional_special_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">values</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">special_tokens_map</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,</span>
<span class="sd">        `unk_token`, etc.) to their values (`&#39;&lt;unk&gt;&#39;`, `&#39;&lt;cls&gt;&#39;`, etc.).</span>

<span class="sd">        Convert potential tokens of `tokenizers.AddedToken` type to string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">attr_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">attr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_value</span><span class="p">:</span>
                <span class="n">set_attr</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">type</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)(</span><span class="nb">str</span><span class="p">(</span><span class="n">attr_value_sub</span><span class="p">)</span> <span class="k">for</span> <span class="n">attr_value_sub</span> <span class="ow">in</span> <span class="n">attr_value</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                    <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">set_attr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">special_tokens_map_extended</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping</span>
<span class="sd">        special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`&#39;&lt;unk&gt;&#39;`, `&#39;&lt;cls&gt;&#39;`, etc.).</span>

<span class="sd">        Don&#39;t convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how</span>
<span class="sd">        special tokens are tokenized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPECIAL_TOKENS_ATTRIBUTES</span><span class="p">:</span>
            <span class="n">attr_value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">attr</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">attr_value</span><span class="p">:</span>
                <span class="n">set_attr</span><span class="p">[</span><span class="n">attr</span><span class="p">]</span> <span class="o">=</span> <span class="n">attr_value</span>
        <span class="k">return</span> <span class="n">set_attr</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `List[str]`: All the special tokens (`&#39;&lt;unk&gt;&#39;`, `&#39;&lt;cls&gt;&#39;`, etc.) mapped to class attributes.</span>

<span class="sd">        Convert tokens of `tokenizers.AddedToken` type to string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens_extended</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">all_toks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_tokens_extended</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`&#39;&lt;unk&gt;&#39;`, `&#39;&lt;cls&gt;&#39;`, etc.) mapped to class</span>
<span class="sd">        attributes.</span>

<span class="sd">        Don&#39;t convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how</span>
<span class="sd">        special tokens are tokenized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">set_attr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span>
        <span class="k">for</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">set_attr</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">all_toks</span> <span class="o">=</span> <span class="n">all_toks</span> <span class="o">+</span> <span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attr_value</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attr_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="k">else</span> <span class="p">[</span><span class="n">attr_value</span><span class="p">])</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">OrderedDict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="n">all_toks</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">all_toks</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">all_special_ids</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `List[int]`: List the ids of the special tokens(`&#39;&lt;unk&gt;&#39;`, `&#39;&lt;cls&gt;&#39;`, etc.) mapped to class attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_toks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="n">all_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">all_toks</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">all_ids</span>


<span class="n">ENCODE_KWARGS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            add_special_tokens (`bool`, *optional*, defaults to `True`):</span>
<span class="s2">                Whether or not to encode the sequences with the special tokens relative to their model.</span>
<span class="s2">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):</span>
<span class="s2">                Activates and controls padding. Accepts the following values:</span>

<span class="s2">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="s2">                  sequence if provided).</span>
<span class="s2">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="s2">                  acceptable input length for the model if that argument is not provided.</span>
<span class="s2">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="s2">                  lengths).</span>
<span class="s2">            truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):</span>
<span class="s2">                Activates and controls truncation. Accepts the following values:</span>

<span class="s2">                - `True` or `&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or</span>
<span class="s2">                  to the maximum acceptable input length for the model if that argument is not provided. This will</span>
<span class="s2">                  truncate token by token, removing a token from the longest sequence in the pair if a pair of</span>
<span class="s2">                  sequences (or a batch of pairs) is provided.</span>
<span class="s2">                - `&#39;only_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="s2">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="s2">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="s2">                - `&#39;only_second&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="s2">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="s2">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="s2">                - `False` or `&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with sequence lengths</span>
<span class="s2">                  greater than the model maximum admissible input size).</span>
<span class="s2">            max_length (`int`, *optional*):</span>
<span class="s2">                Controls the maximum length to use by one of the truncation/padding parameters.</span>

<span class="s2">                If left unset or set to `None`, this will use the predefined model maximum length if a maximum length</span>
<span class="s2">                is required by one of the truncation/padding parameters. If the model has no specific maximum input</span>
<span class="s2">                length (like XLNet) truncation/padding to a maximum length will be deactivated.</span>
<span class="s2">            stride (`int`, *optional*, defaults to 0):</span>
<span class="s2">                If set to a number along with `max_length`, the overflowing tokens returned when</span>
<span class="s2">                `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence</span>
<span class="s2">                returned to provide some overlap between truncated and overflowing sequences. The value of this</span>
<span class="s2">                argument defines the number of overlapping tokens.</span>
<span class="s2">            is_split_into_words (`bool`, *optional*, defaults to `False`):</span>
<span class="s2">                Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the</span>
<span class="s2">                tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)</span>
<span class="s2">                which it will tokenize. This is useful for NER or token classification.</span>
<span class="s2">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="s2">                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable</span>
<span class="s2">                the use of Tensor Cores on NVIDIA hardware with compute capability &gt;= 7.5 (Volta).</span>
<span class="s2">            return_tensors (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="s2">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="s2">                - `&#39;tf&#39;`: Return TensorFlow `tf.constant` objects.</span>
<span class="s2">                - `&#39;pt&#39;`: Return PyTorch `torch.Tensor` objects.</span>
<span class="s2">                - `&#39;np&#39;`: Return Numpy `np.ndarray` objects.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            return_token_type_ids (`bool`, *optional*):</span>
<span class="s2">                Whether to return token type IDs. If left to the default, will return the token type IDs according to</span>
<span class="s2">                the specific tokenizer&#39;s default, defined by the `return_outputs` attribute.</span>

<span class="s2">                [What are token type IDs?](../glossary#token-type-ids)</span>
<span class="s2">            return_attention_mask (`bool`, *optional*):</span>
<span class="s2">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="s2">                to the specific tokenizer&#39;s default, defined by the `return_outputs` attribute.</span>

<span class="s2">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="s2">            return_overflowing_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="s2">                Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch</span>
<span class="s2">                of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead</span>
<span class="s2">                of returning overflowing tokens.</span>
<span class="s2">            return_special_tokens_mask (`bool`, *optional*, defaults to `False`):</span>
<span class="s2">                Whether or not to return special tokens mask information.</span>
<span class="s2">            return_offsets_mapping (`bool`, *optional*, defaults to `False`):</span>
<span class="s2">                Whether or not to return `(char_start, char_end)` for each token.</span>

<span class="s2">                This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using</span>
<span class="s2">                Python&#39;s tokenizer, this method will raise `NotImplementedError`.</span>
<span class="s2">            return_length  (`bool`, *optional*, defaults to `False`):</span>
<span class="s2">                Whether or not to return the lengths of the encoded inputs.</span>
<span class="s2">            verbose (`bool`, *optional*, defaults to `True`):</span>
<span class="s2">                Whether or not to print more information and warnings.</span>
<span class="s2">            **kwargs: passed to the `self.tokenize()` method</span>

<span class="s2">        Return:</span>
<span class="s2">            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:</span>

<span class="s2">            - **input_ids** -- List of token ids to be fed to a model.</span>

<span class="s2">              [What are input IDs?](../glossary#input-ids)</span>

<span class="s2">            - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or</span>
<span class="s2">              if *&quot;token_type_ids&quot;* is in `self.model_input_names`).</span>

<span class="s2">              [What are token type IDs?](../glossary#token-type-ids)</span>

<span class="s2">            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when</span>
<span class="s2">              `return_attention_mask=True` or if *&quot;attention_mask&quot;* is in `self.model_input_names`).</span>

<span class="s2">              [What are attention masks?](../glossary#attention-mask)</span>

<span class="s2">            - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and</span>
<span class="s2">              `return_overflowing_tokens=True`).</span>
<span class="s2">            - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and</span>
<span class="s2">              `return_overflowing_tokens=True`).</span>
<span class="s2">            - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying</span>
<span class="s2">              regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).</span>
<span class="s2">            - **length** -- The length of the inputs (when `return_length=True`)</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">INIT_TOKENIZER_DOCSTRING</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Class attributes (overridden by derived classes)</span>

<span class="s2">        - **vocab_files_names** (`Dict[str, str]`) -- A dictionary with, as keys, the `__init__` keyword name of each</span>
<span class="s2">          vocabulary file required by the model, and as associated values, the filename for saving the associated file</span>
<span class="s2">          (string).</span>
<span class="s2">        - **pretrained_vocab_files_map** (`Dict[str, Dict[str, str]]`) -- A dictionary of dictionaries, with the</span>
<span class="s2">          high-level keys being the `__init__` keyword name of each vocabulary file required by the model, the</span>
<span class="s2">          low-level being the `short-cut-names` of the pretrained models with, as associated values, the `url` to the</span>
<span class="s2">          associated pretrained vocabulary file.</span>
<span class="s2">        - **max_model_input_sizes** (`Dict[str, Optional[int]]`) -- A dictionary with, as keys, the `short-cut-names`</span>
<span class="s2">          of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model,</span>
<span class="s2">          or `None` if the model has no maximum input size.</span>
<span class="s2">        - **pretrained_init_configuration** (`Dict[str, Dict[str, Any]]`) -- A dictionary with, as keys, the</span>
<span class="s2">          `short-cut-names` of the pretrained models, and as associated values, a dictionary of specific arguments to</span>
<span class="s2">          pass to the `__init__` method of the tokenizer class for this pretrained model when loading the tokenizer</span>
<span class="s2">          with the [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`] method.</span>
<span class="s2">        - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.</span>
<span class="s2">        - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.</span>
<span class="s2">          Should be `&#39;right&#39;` or `&#39;left&#39;`.</span>
<span class="s2">        - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation</span>
<span class="s2">          applied. Should be `&#39;right&#39;` or `&#39;left&#39;`.</span>

<span class="s2">    Args:</span>
<span class="s2">        model_max_length (`int`, *optional*):</span>
<span class="s2">            The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is</span>
<span class="s2">            loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the</span>
<span class="s2">            value stored for the associated model in `max_model_input_sizes` (see above). If no value is provided, will</span>
<span class="s2">            default to VERY_LARGE_INTEGER (`int(1e30)`).</span>
<span class="s2">        padding_side (`str`, *optional*):</span>
<span class="s2">            The side on which the model should have padding applied. Should be selected between [&#39;right&#39;, &#39;left&#39;].</span>
<span class="s2">            Default value is picked from the class attribute of the same name.</span>
<span class="s2">        truncation_side (`str`, *optional*):</span>
<span class="s2">            The side on which the model should have truncation applied. Should be selected between [&#39;right&#39;, &#39;left&#39;].</span>
<span class="s2">            Default value is picked from the class attribute of the same name.</span>
<span class="s2">        model_input_names (`List[string]`, *optional*):</span>
<span class="s2">            The list of inputs accepted by the forward pass of the model (like `&quot;token_type_ids&quot;` or</span>
<span class="s2">            `&quot;attention_mask&quot;`). Default value is picked from the class attribute of the same name.</span>
<span class="s2">        bos_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token representing the beginning of a sentence. Will be associated to `self.bos_token` and</span>
<span class="s2">            `self.bos_token_id`.</span>
<span class="s2">        eos_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token representing the end of a sentence. Will be associated to `self.eos_token` and</span>
<span class="s2">            `self.eos_token_id`.</span>
<span class="s2">        unk_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token representing an out-of-vocabulary token. Will be associated to `self.unk_token` and</span>
<span class="s2">            `self.unk_token_id`.</span>
<span class="s2">        sep_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token separating two different sentences in the same input (used by BERT for instance). Will be</span>
<span class="s2">            associated to `self.sep_token` and `self.sep_token_id`.</span>
<span class="s2">        pad_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by</span>
<span class="s2">            attention mechanisms or loss computation. Will be associated to `self.pad_token` and `self.pad_token_id`.</span>
<span class="s2">        cls_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token representing the class of the input (used by BERT for instance). Will be associated to</span>
<span class="s2">            `self.cls_token` and `self.cls_token_id`.</span>
<span class="s2">        mask_token (`str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A special token representing a masked token (used by masked-language modeling pretraining objectives, like</span>
<span class="s2">            BERT). Will be associated to `self.mask_token` and `self.mask_token_id`.</span>
<span class="s2">        additional_special_tokens (tuple or list of `str` or `tokenizers.AddedToken`, *optional*):</span>
<span class="s2">            A tuple or a list of additional special tokens. Add them here to ensure they won&#39;t be split by the</span>
<span class="s2">            tokenization process. Will be associated to `self.additional_special_tokens` and</span>
<span class="s2">            `self.additional_special_tokens_ids`.</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">INIT_TOKENIZER_DOCSTRING</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">PreTrainedTokenizerBase</span><span class="p">(</span><span class="n">SpecialTokensMixin</span><span class="p">,</span> <span class="n">PushToHubMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for [`PreTrainedTokenizer`] and [`PreTrainedTokenizerFast`].</span>

<span class="sd">    Handles shared (mostly boiler plate) methods for those two classes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vocab_files_names</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_vocab_files_map</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">pretrained_init_configuration</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">max_model_input_sizes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">_auto_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># first name has to correspond to main model input name</span>
    <span class="c1"># to make sure `tokenizer.pad(...)` works correctly</span>
    <span class="n">model_input_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;token_type_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
    <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
    <span class="n">truncation_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
    <span class="n">slow_tokenizer_class</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span> <span class="o">=</span> <span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name_or_path&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processor_class</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;processor_class&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># For backward compatibility we fallback to set model_max_length from max_len if provided</span>
        <span class="n">model_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">,</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;max_len&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">=</span> <span class="n">model_max_length</span> <span class="k">if</span> <span class="n">model_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">VERY_LARGE_INTEGER</span>

        <span class="c1"># Padding and truncation side are right by default and overridden in subclasses. If specified in the kwargs, it</span>
        <span class="c1"># is changed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;padding_side&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="s2">&quot;left&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Padding side should be selected between &#39;right&#39; and &#39;left&#39;, current value: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;truncation_side&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="s2">&quot;left&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Padding side should be selected between &#39;right&#39; and &#39;left&#39;, current value: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;model_input_names&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">{}</span>
        <span class="p">)</span>  <span class="c1"># Use to store when we have already noticed a deprecation warning (avoid overlogging).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_target_context_manager</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_len_single_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `int`: The maximum length of a sentence that can be fed to the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_len_sentences_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `int`: The maximum combined length of a pair of sentences that can be fed to the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="nd">@max_len_single_sentence</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">max_len_single_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># For backward compatibility, allow to try to setup &#39;max_len_single_sentence&#39;.</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_len_single_sentence&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Setting &#39;max_len_single_sentence&#39; is now deprecated. This value is automatically set up.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;max_len_single_sentence&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Setting &#39;max_len_single_sentence&#39; is now deprecated. This value is automatically set up.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@max_len_sentences_pair</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">max_len_sentences_pair</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="c1"># For backward compatibility, allow to try to setup &#39;max_len_sentences_pair&#39;.</span>
        <span class="k">if</span> <span class="n">value</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;max_len_sentences_pair&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Setting &#39;max_len_sentences_pair&#39; is now deprecated. This value is automatically set up.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;max_len_sentences_pair&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Setting &#39;max_len_sentences_pair&#39; is now deprecated. This value is automatically set up.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_processor_class</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">processor_class</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets processor class as an attribute.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processor_class</span> <span class="o">=</span> <span class="n">processor_class</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;PreTrainedTokenizerFast&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">is_fast</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;PreTrainedTokenizer&#39;</span><span class="si">}</span><span class="s2">(name_or_path=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name_or_path</span><span class="si">}</span><span class="s2">&#39;,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; vocab_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">, model_max_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2">, is_fast=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">is_fast</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; padding_side=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="si">}</span><span class="s2">&#39;, truncation_side=&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="si">}</span><span class="s2">&#39;,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; special_tokens=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns the vocabulary as a dictionary of token to index.</span>

<span class="sd">        `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the</span>
<span class="sd">        vocab.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Dict[str, int]`: The vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_pretrained</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">pretrained_model_name_or_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span> <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined</span>
<span class="sd">        tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            pretrained_model_name_or_path (`str` or `os.PathLike`):</span>
<span class="sd">                Can be either:</span>

<span class="sd">                - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.</span>
<span class="sd">                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a</span>
<span class="sd">                  user or organization name, like `dbmdz/bert-base-german-cased`.</span>
<span class="sd">                - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved</span>
<span class="sd">                  using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,</span>
<span class="sd">                  `./my_model_directory/`.</span>
<span class="sd">                - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary</span>
<span class="sd">                  file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,</span>
<span class="sd">                  `./my_model_directory/vocab.txt`.</span>
<span class="sd">            cache_dir (`str` or `os.PathLike`, *optional*):</span>
<span class="sd">                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the</span>
<span class="sd">                standard cache should not be used.</span>
<span class="sd">            force_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they</span>
<span class="sd">                exist.</span>
<span class="sd">            resume_download (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to delete incompletely received files. Attempt to resume the download if such a file</span>
<span class="sd">                exists.</span>
<span class="sd">            proxies (`Dict[str, str]`, *optional*):</span>
<span class="sd">                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{&#39;http&#39;: &#39;foo.bar:3128&#39;,</span>
<span class="sd">                &#39;http://hostname&#39;: &#39;foo.bar:4012&#39;}`. The proxies are used on each request.</span>
<span class="sd">            use_auth_token (`str` or *bool*, *optional*):</span>
<span class="sd">                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated</span>
<span class="sd">                when running `huggingface-cli login` (stored in `~/.huggingface`).</span>
<span class="sd">            local_files_only (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to only rely on local files and not to attempt to download any files.</span>
<span class="sd">            revision (`str`, *optional*, defaults to `&quot;main&quot;`):</span>
<span class="sd">                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a</span>
<span class="sd">                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any</span>
<span class="sd">                identifier allowed by git.</span>
<span class="sd">            subfolder (`str`, *optional*):</span>
<span class="sd">                In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for</span>
<span class="sd">                facebook/rag-token-base), specify it here.</span>
<span class="sd">            inputs (additional positional arguments, *optional*):</span>
<span class="sd">                Will be passed along to the Tokenizer `__init__` method.</span>
<span class="sd">            kwargs (additional keyword arguments, *optional*):</span>
<span class="sd">                Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,</span>
<span class="sd">                `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,</span>
<span class="sd">                `additional_special_tokens`. See parameters in the `__init__` for more details.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        Passing `use_auth_token=True` is required when you want to use a private model.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Examples:</span>

<span class="sd">        ```python</span>
<span class="sd">        # We can&#39;t instantiate directly the base class *PreTrainedTokenizerBase* so let&#39;s show our examples on a derived class: BertTokenizer</span>
<span class="sd">        # Download vocabulary from huggingface.co and cache.</span>
<span class="sd">        tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)</span>

<span class="sd">        # Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="sd">        tokenizer = BertTokenizer.from_pretrained(&quot;dbmdz/bert-base-german-cased&quot;)</span>

<span class="sd">        # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#39;./test/saved_model/&#39;)*)</span>
<span class="sd">        tokenizer = BertTokenizer.from_pretrained(&quot;./test/saved_model/&quot;)</span>

<span class="sd">        # If the tokenizer uses a single vocabulary file, you can point directly to this file</span>
<span class="sd">        tokenizer = BertTokenizer.from_pretrained(&quot;./test/saved_model/my_vocab.txt&quot;)</span>

<span class="sd">        # You can link tokens to special vocabulary when instantiating</span>
<span class="sd">        tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;, unk_token=&quot;&lt;unk&gt;&quot;)</span>
<span class="sd">        # You should be sure &#39;&lt;unk&gt;&#39; is in the vocabulary when doing that.</span>
<span class="sd">        # Otherwise use tokenizer.add_special_tokens({&#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;}) instead)</span>
<span class="sd">        assert tokenizer.unk_token == &quot;&lt;unk&gt;&quot;</span>
<span class="sd">        ```&quot;&quot;&quot;</span>
        <span class="n">cache_dir</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cache_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">force_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;force_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">resume_download</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;resume_download&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">proxies</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;proxies&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">local_files_only</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;local_files_only&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">use_auth_token</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;use_auth_token&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">revision</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;revision&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">subfolder</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;subfolder&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_pipeline</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_pipeline&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">from_auto_class</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_from_auto&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_commit_hash&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="n">user_agent</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;file_type&quot;</span><span class="p">:</span> <span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span> <span class="s2">&quot;from_auto_class&quot;</span><span class="p">:</span> <span class="n">from_auto_class</span><span class="p">,</span> <span class="s2">&quot;is_fast&quot;</span><span class="p">:</span> <span class="s2">&quot;Fast&quot;</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">from_pipeline</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">user_agent</span><span class="p">[</span><span class="s2">&quot;using_pipeline&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">from_pipeline</span>

        <span class="k">if</span> <span class="n">is_offline_mode</span><span class="p">()</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">local_files_only</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Offline mode: forcing local_files_only=True&quot;</span><span class="p">)</span>
            <span class="n">local_files_only</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">pretrained_model_name_or_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="n">vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">init_configuration</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="n">is_local</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span>
        <span class="n">single_file_id</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Calling </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.from_pretrained() with the path to a single file or url is not &quot;</span>
                    <span class="s2">&quot;supported for this tokenizer. Use a model identifier or the path to a directory instead.&quot;</span>
                <span class="p">)</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Calling </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.from_pretrained() with the path to a single file or url is deprecated and &quot;</span>
                <span class="s2">&quot;won&#39;t be possible anymore in v5. Use a model identifier or the path to a directory instead.&quot;</span><span class="p">,</span>
                <span class="ne">FutureWarning</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">file_id</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>
            <span class="n">single_file_id</span> <span class="o">=</span> <span class="n">file_id</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># At this point pretrained_model_name_or_path is either a directory or a model identifier name</span>
            <span class="n">additional_files_names</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;added_tokens_file&quot;</span><span class="p">:</span> <span class="n">ADDED_TOKENS_FILE</span><span class="p">,</span>
                <span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">:</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span><span class="p">,</span>
                <span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">:</span> <span class="n">TOKENIZER_CONFIG_FILE</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">vocab_files</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="bp">cls</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="p">,</span> <span class="o">**</span><span class="n">additional_files_names</span><span class="p">}</span>

            <span class="k">if</span> <span class="s2">&quot;tokenizer_file&quot;</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="p">:</span>
                <span class="c1"># Try to get the tokenizer config to see if there are versioned tokenizer files.</span>
                <span class="n">fast_tokenizer_file</span> <span class="o">=</span> <span class="n">FULL_TOKENIZER_FILE</span>
                <span class="n">resolved_config_file</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">TOKENIZER_CONFIG_FILE</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_missing_entries</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_connection_errors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">extract_commit_hash</span><span class="p">(</span><span class="n">resolved_config_file</span><span class="p">,</span> <span class="n">commit_hash</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">resolved_config_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">resolved_config_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
                        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
                        <span class="k">if</span> <span class="s2">&quot;fast_tokenizer_files&quot;</span> <span class="ow">in</span> <span class="n">tokenizer_config</span><span class="p">:</span>
                            <span class="n">fast_tokenizer_file</span> <span class="o">=</span> <span class="n">get_fast_tokenizer_file</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;fast_tokenizer_files&quot;</span><span class="p">])</span>
                <span class="n">vocab_files</span><span class="p">[</span><span class="s2">&quot;tokenizer_file&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fast_tokenizer_file</span>

        <span class="c1"># Get files from url, cache, or disk depending on the case</span>
        <span class="n">resolved_vocab_files</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">unresolved_files</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">file_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">single_file_id</span> <span class="o">==</span> <span class="n">file_id</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
                    <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_path</span>
                <span class="k">elif</span> <span class="n">is_remote_url</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
                    <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">download_url</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">cached_file</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">file_path</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">force_download</span><span class="o">=</span><span class="n">force_download</span><span class="p">,</span>
                    <span class="n">proxies</span><span class="o">=</span><span class="n">proxies</span><span class="p">,</span>
                    <span class="n">resume_download</span><span class="o">=</span><span class="n">resume_download</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                    <span class="n">user_agent</span><span class="o">=</span><span class="n">user_agent</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">subfolder</span><span class="o">=</span><span class="n">subfolder</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_missing_entries</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_raise_exceptions_for_connection_errors</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">commit_hash</span> <span class="o">=</span> <span class="n">extract_commit_hash</span><span class="p">(</span><span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">],</span> <span class="n">commit_hash</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unresolved_files</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t load following files from cache: </span><span class="si">{</span><span class="n">unresolved_files</span><span class="si">}</span><span class="s2"> and cannot check if these &quot;</span>
                <span class="s2">&quot;files are necessary for the tokenizer to operate.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">full_file_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">full_file_name</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
            <span class="k">raise</span> <span class="ne">EnvironmentError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Can&#39;t load tokenizer for &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39;. If you were trying to load it from &quot;</span>
                <span class="s2">&quot;&#39;https://huggingface.co/models&#39;, make sure you don&#39;t have a local directory with the same name. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Otherwise, make sure &#39;</span><span class="si">{</span><span class="n">pretrained_model_name_or_path</span><span class="si">}</span><span class="s2">&#39; is the correct path to a directory &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;containing all relevant files for a </span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> tokenizer.&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">file_id</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">file_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="n">is_local</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading file </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loading file </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s2"> from cache at </span><span class="si">{</span><span class="n">resolved_vocab_files</span><span class="p">[</span><span class="n">file_id</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
            <span class="n">resolved_vocab_files</span><span class="p">,</span>
            <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
            <span class="n">init_configuration</span><span class="p">,</span>
            <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
            <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
            <span class="n">_commit_hash</span><span class="o">=</span><span class="n">commit_hash</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">resolved_vocab_files</span><span class="p">,</span>
        <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
        <span class="n">init_configuration</span><span class="p">,</span>
        <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
        <span class="n">use_auth_token</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">local_files_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">_commit_hash</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="c1"># We instantiate fast tokenizers based on a slow tokenizer if we don&#39;t have access to the tokenizer.json</span>
        <span class="c1"># file or if `from_slow` is set to True.</span>
        <span class="n">from_slow</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;from_slow&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">has_tokenizer_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tokenizer_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">from_slow</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">has_tokenizer_file</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">cls</span><span class="o">.</span><span class="n">slow_tokenizer_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">slow_tokenizer_class</span><span class="p">)</span><span class="o">.</span><span class="n">_from_pretrained</span><span class="p">(</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">resolved_vocab_files</span><span class="p">),</span>
                <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">init_configuration</span><span class="p">),</span>
                <span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span>
                <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                <span class="n">_commit_hash</span><span class="o">=</span><span class="n">_commit_hash</span><span class="p">,</span>
                <span class="o">**</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Prepare tokenizer initialization kwargs</span>
        <span class="c1"># Did we saved some inputs and kwargs to reload ?</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer_config_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tokenizer_config_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tokenizer_config_handle</span><span class="p">:</span>
                <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tokenizer_config_handle</span><span class="p">)</span>
            <span class="c1"># First attempt. We get tokenizer_class from tokenizer_config to check mismatch between tokenizers.</span>
            <span class="n">config_tokenizer_class</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tokenizer_class&quot;</span><span class="p">)</span>
            <span class="n">init_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tokenizer_class&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">init_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;auto_map&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">saved_init_inputs</span> <span class="o">=</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">,</span> <span class="p">())</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">init_inputs</span><span class="p">:</span>
                <span class="n">init_inputs</span> <span class="o">=</span> <span class="n">saved_init_inputs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">config_tokenizer_class</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">init_configuration</span>

        <span class="k">if</span> <span class="n">config_tokenizer_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">.models.auto.configuration_auto</span> <span class="kn">import</span> <span class="n">AutoConfig</span>  <span class="c1"># tests_ignore</span>

            <span class="c1"># Second attempt. If we have not yet found tokenizer_class, let&#39;s try to use the config.</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span>
                    <span class="n">use_auth_token</span><span class="o">=</span><span class="n">use_auth_token</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">local_files_only</span><span class="o">=</span><span class="n">local_files_only</span><span class="p">,</span>
                    <span class="n">_commit_hash</span><span class="o">=</span><span class="n">_commit_hash</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">config_tokenizer_class</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">tokenizer_class</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">OSError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">,</span> <span class="ne">KeyError</span><span class="p">):</span>
                <span class="c1"># skip if an error occurred.</span>
                <span class="n">config</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">config_tokenizer_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Third attempt. If we have not yet found the original type of the tokenizer,</span>
                <span class="c1"># we are loading we see if we can infer it from the type of the configuration file</span>
                <span class="kn">from</span> <span class="nn">.models.auto.tokenization_auto</span> <span class="kn">import</span> <span class="n">TOKENIZER_MAPPING_NAMES</span>  <span class="c1"># tests_ignore</span>

                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;model_type&quot;</span><span class="p">):</span>
                    <span class="n">model_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">model_type</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Fallback: use pattern matching on the string.</span>
                    <span class="n">model_type</span> <span class="o">=</span> <span class="kc">None</span>
                    <span class="k">for</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="n">TOKENIZER_MAPPING_NAMES</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                        <span class="k">if</span> <span class="n">pattern</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">):</span>
                            <span class="n">model_type</span> <span class="o">=</span> <span class="n">pattern</span>
                            <span class="k">break</span>

                <span class="k">if</span> <span class="n">model_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">config_tokenizer_class</span><span class="p">,</span> <span class="n">config_tokenizer_class_fast</span> <span class="o">=</span> <span class="n">TOKENIZER_MAPPING_NAMES</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                        <span class="n">model_type</span><span class="p">,</span> <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">config_tokenizer_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">config_tokenizer_class</span> <span class="o">=</span> <span class="n">config_tokenizer_class_fast</span>

        <span class="k">if</span> <span class="n">config_tokenizer_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;Fast&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="n">config_tokenizer_class</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;Fast&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;The tokenizer class you load from this checkpoint is not the same type as the class this&quot;</span>
                    <span class="s2">&quot; function is called from. It may result in unexpected tokenization. </span><span class="se">\n</span><span class="s2">The tokenizer class you&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; load from this checkpoint is &#39;</span><span class="si">{</span><span class="n">config_tokenizer_class</span><span class="si">}</span><span class="s2">&#39;. </span><span class="se">\n</span><span class="s2">The class this function is called&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; from is &#39;</span><span class="si">{</span><span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Update with newly provided kwargs</span>
        <span class="n">init_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Convert AddedTokens serialized as dict to class instances</span>
        <span class="k">def</span> <span class="nf">convert_added_tokens</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;__type&quot;</span> <span class="ow">in</span> <span class="n">obj</span> <span class="ow">and</span> <span class="n">obj</span><span class="p">[</span><span class="s2">&quot;__type&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;AddedToken&quot;</span><span class="p">:</span>
                <span class="n">obj</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;__type&quot;</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">obj</span>

        <span class="n">init_kwargs</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">init_kwargs</span><span class="p">)</span>

        <span class="c1"># Set max length if needed</span>
        <span class="k">if</span> <span class="n">pretrained_model_name_or_path</span> <span class="ow">in</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">:</span>
            <span class="c1"># if we&#39;re using a pretrained model, ensure the tokenizer</span>
            <span class="c1"># wont index sequences longer than the number of positional embeddings</span>

            <span class="n">model_max_length</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">max_model_input_sizes</span><span class="p">[</span><span class="n">pretrained_model_name_or_path</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">model_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_max_length</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>

                <span class="n">model_max_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e30</span><span class="p">)),</span> <span class="n">model_max_length</span><span class="p">)</span>
                <span class="c1"># TODO(PVP) - uncomment following line in Transformers v5</span>
                <span class="c1"># init_kwargs[&quot;model_max_length&quot;] = model_max_length</span>
                <span class="c1"># TODO(PVP) - remove in Transformers v5</span>
                <span class="c1"># ---</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_eventually_correct_t5_max_length</span><span class="p">(</span>
                    <span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">model_max_length</span><span class="p">,</span> <span class="n">init_kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model_max_length&quot;</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="c1"># ---</span>

        <span class="c1"># Merge resolved_vocab_files arguments in init_kwargs.</span>
        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;added_tokens_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">args_name</span><span class="p">,</span> <span class="n">file_path</span> <span class="ow">in</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">args_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">init_kwargs</span><span class="p">:</span>
                <span class="n">init_kwargs</span><span class="p">[</span><span class="n">args_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_path</span>

        <span class="k">if</span> <span class="n">slow_tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;__slow_tokenizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">slow_tokenizer</span>

        <span class="n">init_kwargs</span><span class="p">[</span><span class="s2">&quot;name_or_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pretrained_model_name_or_path</span>

        <span class="c1"># Instantiate tokenizer.</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="o">*</span><span class="n">init_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span>
                <span class="s2">&quot;Unable to load vocabulary from file. &quot;</span>
                <span class="s2">&quot;Please check that the provided vocabulary is accessible and not corrupted.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Save inputs and kwargs for saving and re-loading with ``save_pretrained``</span>
        <span class="c1"># Removed: Now done at the base class level</span>
        <span class="c1"># tokenizer.init_inputs = init_inputs</span>
        <span class="c1"># tokenizer.init_kwargs = init_kwargs</span>

        <span class="c1"># If there is a complementary special token map, load it</span>
        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">resolved_vocab_files</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;special_tokens_map_file&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">special_tokens_map_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">special_tokens_map_handle</span><span class="p">:</span>
                <span class="n">special_tokens_map</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">special_tokens_map_handle</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">special_tokens_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="n">kwargs</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                    <span class="c1"># This value has already been redefined by the kwargs</span>
                    <span class="c1"># We keep this new value and ignore the one stored in the special_tokens_map_file</span>

                    <span class="k">continue</span>

                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">value</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">AddedToken</span><span class="p">(</span><span class="o">**</span><span class="n">token</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">value</span><span class="p">]</span>
                <span class="nb">setattr</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

        <span class="c1"># Add supplementary tokens.</span>
        <span class="n">special_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">all_special_tokens</span>
        <span class="k">if</span> <span class="n">added_tokens_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">added_tokens_handle</span><span class="p">:</span>
                <span class="n">added_tok_encoder</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">added_tokens_handle</span><span class="p">)</span>

            <span class="c1"># Sort added tokens by index</span>
            <span class="n">added_tok_encoder_sorted</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">added_tok_encoder</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

            <span class="c1"># Accumulate added tokens into batches of special/non-special tokens, because calling add_tokens() for</span>
            <span class="c1"># individual tokens would repeatedly rebuild a trie, which can be slow.</span>
            <span class="n">is_last_special</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">added_tok_encoder_sorted</span><span class="p">:</span>
                <span class="n">current_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">has_tokenizer_file</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">current_index</span> <span class="ow">and</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">!=</span> <span class="n">index</span><span class="p">:</span>
                    <span class="c1"># Tokenizer fast: added token needs to either be in the vocabulary with the proper index or the</span>
                    <span class="c1"># index is the current length of the tokenizer (not in vocabulary)</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Wrong index found for </span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">: should be </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="si">}</span><span class="s2"> but found &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="ow">not</span> <span class="n">has_tokenizer_file</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">!=</span> <span class="n">current_index</span><span class="p">:</span>
                    <span class="c1"># Tokenizer slow: added token cannot already be in the vocabulary so its index needs to be the</span>
                    <span class="c1"># current length of the tokenizer.</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Non-consecutive added token &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; found. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;Should have index </span><span class="si">{</span><span class="n">current_index</span><span class="si">}</span><span class="s2"> but has index </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2"> in saved vocabulary.&quot;</span>
                    <span class="p">)</span>

                <span class="n">is_special</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">token</span> <span class="ow">in</span> <span class="n">special_tokens</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_last_special</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">is_last_special</span> <span class="o">==</span> <span class="n">is_special</span><span class="p">:</span>
                    <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">is_last_special</span><span class="p">)</span>
                    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="p">]</span>
                <span class="n">is_last_special</span> <span class="o">=</span> <span class="n">is_special</span>

            <span class="k">if</span> <span class="n">tokens</span><span class="p">:</span>
                <span class="n">tokenizer</span><span class="o">.</span><span class="n">add_tokens</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">is_last_special</span><span class="p">)</span>

        <span class="c1"># Check all our special tokens are registered as &quot;no split&quot; token (we don&#39;t cut them) and are in the vocab</span>
        <span class="n">added_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">sanitize_special_tokens</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">added_tokens</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_advice</span><span class="p">(</span>
                <span class="s2">&quot;Special tokens have been added in the vocabulary, make sure the associated word embeddings are&quot;</span>
                <span class="s2">&quot; fine-tuned or trained.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">tokenizer</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_eventually_correct_t5_max_length</span><span class="p">(</span><span class="n">pretrained_model_name_or_path</span><span class="p">,</span> <span class="n">max_model_length</span><span class="p">,</span> <span class="n">init_max_model_length</span><span class="p">):</span>
        <span class="c1"># This method should be deleted in Transformers v5</span>
        <span class="c1"># Its only purpose is to potentially throw a warning</span>
        <span class="c1"># that incorrectly defined max lengths of T5&#39;s tokenizer are used</span>
        <span class="c1"># which we will correct in Transformers v5.</span>
        <span class="k">return</span> <span class="n">max_model_length</span>

    <span class="k">def</span> <span class="nf">save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">legacy_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">push_to_hub</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save the full tokenizer state.</span>


<span class="sd">        This method make sure the full tokenizer can then be re-loaded using the</span>
<span class="sd">        [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..</span>

<span class="sd">        Warning,None This won&#39;t save modifications you may have applied to the tokenizer after the instantiation (for</span>
<span class="sd">        instance, modifying `tokenizer.do_lower_case` after creation).</span>

<span class="sd">        Args:</span>
<span class="sd">            save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.</span>
<span class="sd">            legacy_format (`bool`, *optional*):</span>
<span class="sd">                Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON</span>
<span class="sd">                format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate</span>
<span class="sd">                added_tokens files.</span>

<span class="sd">                If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with</span>
<span class="sd">                &quot;slow&quot; tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be</span>
<span class="sd">                loaded in the corresponding &quot;slow&quot; tokenizer.</span>

<span class="sd">                If `True`, will save the tokenizer in legacy format. If the &quot;slow&quot; tokenizer doesn&#39;t exits, a value</span>
<span class="sd">                error is raised.</span>
<span class="sd">            filename_prefix: (`str`, *optional*):</span>
<span class="sd">                A prefix to add to the names of the files saved by the tokenizer.</span>
<span class="sd">            push_to_hub (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the</span>
<span class="sd">                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your</span>
<span class="sd">                namespace).</span>
<span class="sd">            kwargs:</span>
<span class="sd">                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of `str`: The files saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">save_directory</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Provided path (</span><span class="si">{</span><span class="n">save_directory</span><span class="si">}</span><span class="s2">) should be a directory, not a file&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">push_to_hub</span><span class="p">:</span>
            <span class="n">commit_message</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;commit_message&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;repo_id&quot;</span><span class="p">,</span> <span class="n">save_directory</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">sep</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">repo_id</span><span class="p">,</span> <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_repo</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">files_timestamps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_files_timestamps</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="n">special_tokens_map_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">SPECIAL_TOKENS_MAP_FILE</span>
        <span class="p">)</span>
        <span class="n">tokenizer_config_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">TOKENIZER_CONFIG_FILE</span>
        <span class="p">)</span>

        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;init_inputs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_inputs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">file_id</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_files_names</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">tokenizer_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">file_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Sanitize AddedTokens</span>
        <span class="k">def</span> <span class="nf">convert_added_tokens</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AddedToken</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">AddedToken</span><span class="p">):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">obj</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">add_type_field</span><span class="p">:</span>
                    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;__type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;AddedToken&quot;</span>
                <span class="k">return</span> <span class="n">out</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="n">add_type_field</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="n">add_type_field</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="k">return</span> <span class="n">obj</span>

        <span class="c1"># add_type_field=True to allow dicts in the kwargs / differentiate from AddedToken serialization</span>
        <span class="n">tokenizer_config</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Add tokenizer class to the tokenizer config to be able to reload it with from_pretrained</span>
        <span class="n">tokenizer_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="c1"># Remove the Fast at the end unless we have a special `PreTrainedTokenizerFast`</span>
        <span class="k">if</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;Fast&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tokenizer_class</span> <span class="o">!=</span> <span class="s2">&quot;PreTrainedTokenizerFast&quot;</span><span class="p">:</span>
            <span class="n">tokenizer_class</span> <span class="o">=</span> <span class="n">tokenizer_class</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
        <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;tokenizer_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer_class</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_auto_map&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;auto_map&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_map</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_processor_class&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokenizer_config</span><span class="p">[</span><span class="s2">&quot;processor_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processor_class</span>

        <span class="c1"># If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be</span>
        <span class="c1"># loaded from the Hub.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_auto_class</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">custom_object_save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">tokenizer_config</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">out_str</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tokenizer_config</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out_str</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;tokenizer config file saved in </span><span class="si">{</span><span class="n">tokenizer_config_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Sanitize AddedTokens in special_tokens_map</span>
        <span class="n">write_dict</span> <span class="o">=</span> <span class="n">convert_added_tokens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">special_tokens_map_extended</span><span class="p">,</span> <span class="n">add_type_field</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">special_tokens_map_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">out_str</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">write_dict</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out_str</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Special tokens file saved in </span><span class="si">{</span><span class="n">special_tokens_map_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">file_names</span> <span class="o">=</span> <span class="p">(</span><span class="n">tokenizer_config_file</span><span class="p">,</span> <span class="n">special_tokens_map_file</span><span class="p">)</span>

        <span class="n">save_files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_pretrained</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="o">=</span><span class="n">save_directory</span><span class="p">,</span>
            <span class="n">file_names</span><span class="o">=</span><span class="n">file_names</span><span class="p">,</span>
            <span class="n">legacy_format</span><span class="o">=</span><span class="n">legacy_format</span><span class="p">,</span>
            <span class="n">filename_prefix</span><span class="o">=</span><span class="n">filename_prefix</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">push_to_hub</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_upload_modified_files</span><span class="p">(</span>
                <span class="n">save_directory</span><span class="p">,</span> <span class="n">repo_id</span><span class="p">,</span> <span class="n">files_timestamps</span><span class="p">,</span> <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">token</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">save_files</span>

    <span class="k">def</span> <span class="nf">_save_pretrained</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">save_directory</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">file_names</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">legacy_format</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens.</span>

<span class="sd">        Fast tokenizers can also be saved in a unique JSON file containing {config + vocab + added-tokens} using the</span>
<span class="sd">        specific [`~tokenization_utils_fast.PreTrainedTokenizerFast._save_pretrained`]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">legacy_format</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Only fast tokenizers (instances of PreTrainedTokenizerFast) can be saved in non legacy format.&quot;</span>
            <span class="p">)</span>

        <span class="n">save_directory</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>

        <span class="n">added_tokens_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="n">save_directory</span><span class="p">,</span> <span class="p">(</span><span class="n">filename_prefix</span> <span class="o">+</span> <span class="s2">&quot;-&quot;</span> <span class="k">if</span> <span class="n">filename_prefix</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">ADDED_TOKENS_FILE</span>
        <span class="p">)</span>
        <span class="n">added_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_added_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">added_vocab</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">out_str</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">added_vocab</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ensure_ascii</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">out_str</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;added tokens file saved in </span><span class="si">{</span><span class="n">added_tokens_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">vocab_files</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_vocabulary</span><span class="p">(</span><span class="n">save_directory</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="o">=</span><span class="n">filename_prefix</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">file_names</span> <span class="o">+</span> <span class="n">vocab_files</span> <span class="o">+</span> <span class="p">(</span><span class="n">added_tokens_file</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">save_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_directory</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filename_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save only the vocabulary of the tokenizer (vocabulary + added tokens).</span>

<span class="sd">        This method won&#39;t save the configuration and special token mappings of the tokenizer. Use</span>
<span class="sd">        [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.</span>

<span class="sd">        Args:</span>
<span class="sd">            save_directory (`str`):</span>
<span class="sd">                The directory in which to save the vocabulary.</span>
<span class="sd">            filename_prefix (`str`, *optional*):</span>
<span class="sd">                An optional prefix to add to the named of the saved files.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Tuple(str)`: Paths to the files saved.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string in a sequence of tokens, replacing unknown tokens with the `unk_token`.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (`str`):</span>
<span class="sd">                The sequence to be encoded.</span>
<span class="sd">            pair (`str`, *optional*):</span>
<span class="sd">                A second sequence to be encoded with the first.</span>
<span class="sd">            add_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to add the special tokens associated with the corresponding model.</span>
<span class="sd">            kwargs (additional keyword arguments, *optional*):</span>
<span class="sd">                Will be passed to the underlying model specific encode method. See details in</span>
<span class="sd">                [`~PreTrainedTokenizerBase.__call__`]</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[str]`: The list of tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@add_end_docstrings</span><span class="p">(</span>
        <span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            **kwargs: Passed along to the `.tokenize()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.</span>
<span class="sd">        &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.</span>

<span class="sd">        Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]` or `List[int]`):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the</span>
<span class="sd">                `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method).</span>
<span class="sd">            text_pair (`str`, `List[str]` or `List[int]`, *optional*):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">num_special_tokens_to_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_get_padding_truncation_strategies</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the correct padding/truncation strategy with backward compatibility for old arguments (truncation_strategy</span>
<span class="sd">        and pad_to_max_length) and behaviors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">old_truncation_strategy</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;truncation_strategy&quot;</span><span class="p">,</span> <span class="s2">&quot;do_not_truncate&quot;</span><span class="p">)</span>
        <span class="n">old_pad_to_max_length</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;pad_to_max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Backward compatibility for previous behavior, maybe we should deprecate it:</span>
        <span class="c1"># If you only set max_length, it activates truncation for max_length</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Truncation-not-explicitly-activated&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;Truncation was not explicitly activated but `max_length` is provided a specific value, please&quot;</span>
                        <span class="s2">&quot; use `truncation=True` to explicitly truncate examples to max length. Defaulting to&quot;</span>
                        <span class="s2">&quot; &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the&quot;</span>
                        <span class="s2">&quot; tokenizer you can select this strategy more precisely by providing a specific strategy to&quot;</span>
                        <span class="s2">&quot; `truncation`.&quot;</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Truncation-not-explicitly-activated&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">truncation</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span>

        <span class="c1"># Get padding strategy</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">old_pad_to_max_length</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;The `pad_to_max_length` argument is deprecated and will be removed in a future version, &quot;</span>
                    <span class="s2">&quot;use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or &quot;</span>
                    <span class="s2">&quot;use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific &quot;</span>
                    <span class="s2">&quot;length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the &quot;</span>
                    <span class="s2">&quot;maximal input size of the model (e.g. 512 for Bert).&quot;</span><span class="p">,</span>
                    <span class="ne">FutureWarning</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span>
        <span class="k">elif</span> <span class="n">padding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">padding</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
                        <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">False</span> <span class="ow">or</span> <span class="n">truncation</span> <span class="o">==</span> <span class="s2">&quot;do_not_truncate&quot;</span>
                    <span class="p">):</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                            <span class="s2">&quot;`max_length` is ignored when `padding`=`True` and there is no truncation strategy. &quot;</span>
                            <span class="s2">&quot;To pad to max length, use `padding=&#39;max_length&#39;`.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">old_pad_to_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span><span class="p">:</span>
                        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Though `pad_to_max_length` = `True`, it is ignored because `padding`=`True`.&quot;</span><span class="p">)</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span>  <span class="c1"># Default to pad to the longest sequence in the batch</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">):</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">):</span>
                <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>

        <span class="c1"># Get truncation strategy</span>
        <span class="k">if</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">old_truncation_strategy</span> <span class="o">!=</span> <span class="s2">&quot;do_not_truncate&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;The `truncation_strategy` argument is deprecated and will be removed in a future version, use&quot;</span>
                    <span class="s2">&quot; `truncation=True` to truncate examples to a max length. You can give a specific length with&quot;</span>
                    <span class="s2">&quot; `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input&quot;</span>
                    <span class="s2">&quot; size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific&quot;</span>
                    <span class="s2">&quot; truncation strategy selected among `truncation=&#39;only_first&#39;` (will only truncate the first&quot;</span>
                    <span class="s2">&quot; sentence in the pairs) `truncation=&#39;only_second&#39;` (will only truncate the second sentence in the&quot;</span>
                    <span class="s2">&quot; pairs) or `truncation=&#39;longest_first&#39;` (will iteratively remove tokens from the longest sentence&quot;</span>
                    <span class="s2">&quot; in the pairs).&quot;</span><span class="p">,</span>
                    <span class="ne">FutureWarning</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">old_truncation_strategy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">False</span> <span class="ow">and</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">truncation</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span>
                <span class="p">)</span>  <span class="c1"># Default to truncate the longest sequences in pairs of inputs</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">truncation</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
                <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">truncation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>

        <span class="c1"># Set max length if needed</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">&gt;</span> <span class="n">LARGE_INTEGER</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Asking-to-pad-to-max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                <span class="s2">&quot;Asking to pad to max_length but no maximum length is provided and the model has no&quot;</span>
                                <span class="s2">&quot; predefined maximum length. Default to no padding.&quot;</span>
                            <span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Asking-to-pad-to-max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>

            <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="o">&gt;</span> <span class="n">LARGE_INTEGER</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Asking-to-truncate-to-max_length&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                <span class="s2">&quot;Asking to truncate to max_length but no maximum length is provided and the model has&quot;</span>
                                <span class="s2">&quot; no predefined maximum length. Default to no truncation.&quot;</span>
                            <span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Asking-to-truncate-to-max_length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>

        <span class="c1"># Test if we have a padding token</span>
        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Asking to pad but the tokenizer does not have a padding token. &quot;</span>
                <span class="s2">&quot;Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` &quot;</span>
                <span class="s2">&quot;or add a new pad token via `tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check that we will truncate to a multiple of pad_to_multiple_of if both are provided</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span>
            <span class="ow">and</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span>
            <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">%</span> <span class="n">pad_to_multiple_of</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Truncation and padding are both activated but &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;truncation length (</span><span class="si">{</span><span class="n">max_length</span><span class="si">}</span><span class="s2">) is not a multiple of pad_to_multiple_of (</span><span class="si">{</span><span class="n">pad_to_multiple_of</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span>

    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_target</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">text_pair_target</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of</span>
<span class="sd">        sequences.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings</span>
<span class="sd">                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set</span>
<span class="sd">                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">            text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):</span>
<span class="sd">                The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a</span>
<span class="sd">                list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),</span>
<span class="sd">                you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># To avoid duplicating</span>
        <span class="n">all_kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">all_kwargs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">text_target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;You need to specify either `text` or `text_target`.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># The context manager will send the inputs as normal texts and not text_target, but we shouldn&#39;t change the</span>
            <span class="c1"># input mode in this case.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_in_target_context_manager</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_input_mode</span><span class="p">()</span>
            <span class="n">encodings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_one</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span> <span class="o">**</span><span class="n">all_kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">text_target</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_target_mode</span><span class="p">()</span>
            <span class="n">target_encodings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_one</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text_target</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair_target</span><span class="p">,</span> <span class="o">**</span><span class="n">all_kwargs</span><span class="p">)</span>
        <span class="c1"># Leave back tokenizer in input mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_input_mode</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">text_target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">encodings</span>
        <span class="k">elif</span> <span class="n">text</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">target_encodings</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">encodings</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_encodings</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">encodings</span>

    <span class="k">def</span> <span class="nf">_call_one</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="c1"># Input type checking for clearer error</span>
        <span class="k">def</span> <span class="nf">_is_valid_text_input</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># Strings are fine</span>
                <span class="k">return</span> <span class="kc">True</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="c1"># List are fine as long as they are...</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># ... empty</span>
                    <span class="k">return</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                    <span class="c1"># ... list of strings</span>
                    <span class="k">return</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                    <span class="c1"># ... list with an empty list or with a list of strings</span>
                    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_is_valid_text_input</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;</span>
                <span class="s2">&quot;or `List[List[str]]` (batch of pretokenized examples).&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_is_valid_text_input</span><span class="p">(</span><span class="n">text_pair</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;</span>
                <span class="s2">&quot;or `List[List[str]]` (batch of pretokenized examples).&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_split_into_words</span><span class="p">:</span>
            <span class="n">is_batched</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">text</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">is_batched</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">is_batched</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_pair</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;when tokenizing batches of text, `text_pair` must be a list or tuple with the same length as&quot;</span>
                    <span class="s2">&quot; `text`.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_pair</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;batch length of `text`: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="si">}</span><span class="s2"> does not match batch length of `text_pair`:&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">text_pair</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">batch_text_or_text_pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">))</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">text</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_encode_plus</span><span class="p">(</span>
                <span class="n">batch_text_or_text_pairs</span><span class="o">=</span><span class="n">batch_text_or_text_pairs</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
                <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
                <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
                <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
                <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
                <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
                <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
                <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
                <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
                <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize and prepare for the model a sequence or a pair of sequences.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This method is deprecated, `__call__` should be used instead.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            text (`str`, `List[str]` or `List[int]` (the latter only for not-fast tokenizers)):</span>
<span class="sd">                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the</span>
<span class="sd">                `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method).</span>
<span class="sd">            text_pair (`str`, `List[str]` or `List[int]`, *optional*):</span>
<span class="sd">                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using</span>
<span class="sd">                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`</span>
<span class="sd">                method).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">TextInput</span><span class="p">,</span> <span class="n">PreTokenizedInput</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">TruncationStrategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInputPair</span><span class="p">],</span>
        <span class="p">],</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This method is deprecated, `__call__` should be used instead.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):</span>
<span class="sd">                Batch of sequences or pair of sequences to be encoded. This can be a list of</span>
<span class="sd">                string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see</span>
<span class="sd">                details in `encode_plus`).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_encode_plus</span><span class="p">(</span>
            <span class="n">batch_text_or_text_pairs</span><span class="o">=</span><span class="n">batch_text_or_text_pairs</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span><span class="p">,</span>
            <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
            <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">return_token_type_ids</span><span class="o">=</span><span class="n">return_token_type_ids</span><span class="p">,</span>
            <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="n">return_overflowing_tokens</span><span class="o">=</span><span class="n">return_overflowing_tokens</span><span class="p">,</span>
            <span class="n">return_special_tokens_mask</span><span class="o">=</span><span class="n">return_special_tokens_mask</span><span class="p">,</span>
            <span class="n">return_offsets_mapping</span><span class="o">=</span><span class="n">return_offsets_mapping</span><span class="p">,</span>
            <span class="n">return_length</span><span class="o">=</span><span class="n">return_length</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_batch_encode_plus</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_text_or_text_pairs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">TextInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">PreTokenizedInputPair</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">EncodedInputPair</span><span class="p">],</span>
        <span class="p">],</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">TruncationStrategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">pad</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoded_inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">BatchEncoding</span><span class="p">,</span>
            <span class="n">List</span><span class="p">[</span><span class="n">BatchEncoding</span><span class="p">],</span>
            <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span>
            <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">EncodedInput</span><span class="p">]],</span>
            <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">]],</span>
        <span class="p">],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length</span>
<span class="sd">        in the batch.</span>

<span class="sd">        Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,</span>
<span class="sd">        `self.pad_token_id` and `self.pad_token_type_id`).</span>

<span class="sd">        Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the</span>
<span class="sd">        text followed by a call to the `pad` method to get a padded encoding.</span>

<span class="sd">        &lt;Tip&gt;</span>

<span class="sd">        If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the</span>
<span class="sd">        result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of</span>
<span class="sd">        PyTorch tensors, you will lose the specific device of your tensors however.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):</span>
<span class="sd">                Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of</span>
<span class="sd">                tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,</span>
<span class="sd">                List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader</span>
<span class="sd">                collate function.</span>

<span class="sd">                Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see</span>
<span class="sd">                the note above for the return type.</span>
<span class="sd">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">                 Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding</span>
<span class="sd">                 index) among:</span>

<span class="sd">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">                  sequence if provided).</span>
<span class="sd">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">                  acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">                  lengths).</span>
<span class="sd">            max_length (`int`, *optional*):</span>
<span class="sd">                Maximum length of the returned list and optionally padding length (see above).</span>
<span class="sd">            pad_to_multiple_of (`int`, *optional*):</span>
<span class="sd">                If set will pad the sequence to a multiple of the provided value.</span>

<span class="sd">                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</span>
<span class="sd">                &gt;= 7.5 (Volta).</span>
<span class="sd">            return_attention_mask (`bool`, *optional*):</span>
<span class="sd">                Whether to return the attention mask. If left to the default, will return the attention mask according</span>
<span class="sd">                to the specific tokenizer&#39;s default, defined by the `return_outputs` attribute.</span>

<span class="sd">                [What are attention masks?](../glossary#attention-mask)</span>
<span class="sd">            return_tensors (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="sd">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">                - `&#39;tf&#39;`: Return TensorFlow `tf.constant` objects.</span>
<span class="sd">                - `&#39;pt&#39;`: Return PyTorch `torch.Tensor` objects.</span>
<span class="sd">                - `&#39;np&#39;`: Return Numpy `np.ndarray` objects.</span>
<span class="sd">            verbose (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to print more information and warnings.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;Fast&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;Asking-to-pad-a-fast-tokenizer&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning_advice</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;You&#39;re using a </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> tokenizer. Please note that with a fast tokenizer,&quot;</span>
                    <span class="s2">&quot; using the `__call__` method is faster than using a method to encode the text followed by a call&quot;</span>
                    <span class="s2">&quot; to the `pad` method to get a padded encoding.&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;Asking-to-pad-a-fast-tokenizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># If we have a list of dicts, let&#39;s convert it in a dict of lists</span>
        <span class="c1"># We do this to allow using this method as a collate_fn function in PyTorch Dataloader</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Mapping</span><span class="p">):</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">[</span><span class="n">example</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>

        <span class="c1"># The model&#39;s main input name, usually `input_ids`, has be passed for padding</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You should supply an encoding or a list of encodings to this method &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;that includes </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, but you provided </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">required_input</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">required_input</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">return</span> <span class="n">encoded_inputs</span>

        <span class="c1"># If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects</span>
        <span class="c1"># and rebuild them afterwards if no return_tensors is specified</span>
        <span class="c1"># Note that we lose the specific device the tensor may be on for PyTorch</span>

        <span class="n">first_element</span> <span class="o">=</span> <span class="n">required_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="c1"># first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">required_input</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">first_element</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="k">break</span>
        <span class="c1"># At this state, if `first_element` is still a list/tuple, it&#39;s an empty one so there is nothing to do.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">is_tf_tensor</span><span class="p">(</span><span class="n">first_element</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;tf&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">elif</span> <span class="n">is_torch_tensor</span><span class="p">(</span><span class="n">first_element</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">first_element</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;np&quot;</span> <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">return_tensors</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;type of </span><span class="si">{</span><span class="n">first_element</span><span class="si">}</span><span class="s2"> unknown: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">first_element</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Should be one of a python, numpy, pytorch or tensorflow object.&quot;</span>
                <span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">to_py_obj</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="c1"># Convert padding_strategy in PaddingStrategy</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="p">)</span>

        <span class="n">required_input</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="k">if</span> <span class="n">required_input</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">required_input</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad</span><span class="p">(</span>
                <span class="n">encoded_inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
        <span class="p">),</span> <span class="s2">&quot;Some items in the output dictionary have a different batch size than others.&quot;</span>

        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="k">for</span> <span class="n">inputs</span> <span class="ow">in</span> <span class="n">required_input</span><span class="p">)</span>
            <span class="n">padding_strategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">MAX_LENGTH</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pad</span><span class="p">(</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding_strategy</span><span class="o">=</span><span class="n">padding_strategy</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">batch_outputs</span><span class="p">:</span>
                    <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">BatchEncoding</span><span class="p">(</span><span class="n">batch_outputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_token_type_ids_from_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create the token type IDs corresponding to the sequences passed. [What are token type</span>
<span class="sd">        IDs?](../glossary#token-type-ids)</span>

<span class="sd">        Should be overridden in a subclass if the model has a special way of building those.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`): The first tokenized sequence.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: The token type ids.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">*</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_0</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids_1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_inputs_with_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and</span>
<span class="sd">        adding special tokens.</span>

<span class="sd">        This implementation does not add special tokens and this method should be overridden in a subclass.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`): The first tokenized sequence.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[int]`: The model input with special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">token_ids_0</span>
        <span class="k">return</span> <span class="n">token_ids_0</span> <span class="o">+</span> <span class="n">token_ids_1</span>

    <span class="nd">@add_end_docstrings</span><span class="p">(</span><span class="n">ENCODE_KWARGS_DOCSTRING</span><span class="p">,</span> <span class="n">ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">prepare_for_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_token_type_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_overflowing_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_special_tokens_mask</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_offsets_mapping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">return_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prepend_batch_axis</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It</span>
<span class="sd">        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and</span>
<span class="sd">        manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*</span>
<span class="sd">        different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return</span>
<span class="sd">        overflowing tokens. Such a combination of arguments will raise an error.</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (`List[int]`):</span>
<span class="sd">                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and</span>
<span class="sd">                `convert_tokens_to_ids` methods.</span>
<span class="sd">            pair_ids (`List[int]`, *optional*):</span>
<span class="sd">                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`</span>
<span class="sd">                and `convert_tokens_to_ids` methods.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Backward compatibility for &#39;truncation_strategy&#39;, &#39;pad_to_max_length&#39;</span>
        <span class="n">padding_strategy</span><span class="p">,</span> <span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_padding_truncation_strategies</span><span class="p">(</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">pair</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">len_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span>
        <span class="n">len_pair_ids</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Asking to return token_type_ids while setting add_special_tokens to False &quot;</span>
                <span class="s2">&quot;results in an undefined behavior. Please set add_special_tokens to True or &quot;</span>
                <span class="s2">&quot;set return_token_type_ids to None.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">return_overflowing_tokens</span>
            <span class="ow">and</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span>
            <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Not possible to return overflowing tokens for pair of sequences with the &quot;</span>
                <span class="s2">&quot;`longest_first`. Please select another truncation strategy than `longest_first`, &quot;</span>
                <span class="s2">&quot;for instance `only_second` or `only_first`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Load from model defaults</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_token_type_ids</span> <span class="o">=</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Compute the total size of the returned encodings</span>
        <span class="n">total_len</span> <span class="o">=</span> <span class="n">len_ids</span> <span class="o">+</span> <span class="n">len_pair_ids</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_special_tokens_to_add</span><span class="p">(</span><span class="n">pair</span><span class="o">=</span><span class="n">pair</span><span class="p">)</span> <span class="k">if</span> <span class="n">add_special_tokens</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Truncation: Handle max sequence length</span>
        <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">!=</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">DO_NOT_TRUNCATE</span> <span class="ow">and</span> <span class="n">max_length</span> <span class="ow">and</span> <span class="n">total_len</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncate_sequences</span><span class="p">(</span>
                <span class="n">ids</span><span class="p">,</span>
                <span class="n">pair_ids</span><span class="o">=</span><span class="n">pair_ids</span><span class="p">,</span>
                <span class="n">num_tokens_to_remove</span><span class="o">=</span><span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span>
                <span class="n">truncation_strategy</span><span class="o">=</span><span class="n">truncation_strategy</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_overflowing_tokens</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;overflowing_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">overflowing_tokens</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;num_truncated_tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">total_len</span> <span class="o">-</span> <span class="n">max_length</span>

        <span class="c1"># Add special tokens</span>
        <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_token_type_ids_from_sequences</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sequence</span> <span class="o">=</span> <span class="n">ids</span> <span class="o">+</span> <span class="n">pair_ids</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="n">ids</span>
            <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">+</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">pair</span> <span class="k">else</span> <span class="p">[])</span>

        <span class="c1"># Build output dictionary</span>
        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sequence</span>
        <span class="k">if</span> <span class="n">return_token_type_ids</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_type_ids</span>
        <span class="k">if</span> <span class="n">return_special_tokens_mask</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">add_special_tokens</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_special_tokens_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>

        <span class="c1"># Check lengths</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>

        <span class="c1"># Padding</span>
        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">or</span> <span class="n">return_attention_mask</span><span class="p">:</span>
            <span class="n">encoded_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">encoded_inputs</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding_strategy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                <span class="n">pad_to_multiple_of</span><span class="o">=</span><span class="n">pad_to_multiple_of</span><span class="p">,</span>
                <span class="n">return_attention_mask</span><span class="o">=</span><span class="n">return_attention_mask</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">return_length</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>

        <span class="n">batch_outputs</span> <span class="o">=</span> <span class="n">BatchEncoding</span><span class="p">(</span>
            <span class="n">encoded_inputs</span><span class="p">,</span> <span class="n">tensor_type</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span> <span class="n">prepend_batch_axis</span><span class="o">=</span><span class="n">prepend_batch_axis</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_outputs</span>

    <span class="k">def</span> <span class="nf">truncate_sequences</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">pair_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_tokens_to_remove</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">truncation_strategy</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;longest_first&quot;</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Truncates a sequence pair in-place following the strategy.</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (`List[int]`):</span>
<span class="sd">                Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and</span>
<span class="sd">                `convert_tokens_to_ids` methods.</span>
<span class="sd">            pair_ids (`List[int]`, *optional*):</span>
<span class="sd">                Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`</span>
<span class="sd">                and `convert_tokens_to_ids` methods.</span>
<span class="sd">            num_tokens_to_remove (`int`, *optional*, defaults to 0):</span>
<span class="sd">                Number of tokens to remove using the truncation strategy.</span>
<span class="sd">            truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):</span>
<span class="sd">                The strategy to follow for truncation. Can be:</span>

<span class="sd">                - `&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided. This will truncate</span>
<span class="sd">                  token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a</span>
<span class="sd">                  batch of pairs) is provided.</span>
<span class="sd">                - `&#39;only_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                - `&#39;only_second&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                - `&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with sequence lengths greater</span>
<span class="sd">                  than the model maximum admissible input size).</span>
<span class="sd">            stride (`int`, *optional*, defaults to 0):</span>
<span class="sd">                If set to a positive number, the overflowing tokens returned will contain some tokens from the main</span>
<span class="sd">                sequence returned. The value of this argument defines the number of additional tokens.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of</span>
<span class="sd">            overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair</span>
<span class="sd">            of sequences (or a batch of pairs) is provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_tokens_to_remove</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">truncation_strategy</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">):</span>
            <span class="n">truncation_strategy</span> <span class="o">=</span> <span class="n">TruncationStrategy</span><span class="p">(</span><span class="n">truncation_strategy</span><span class="p">)</span>

        <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">ONLY_FIRST</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span> <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span><span class="p">:</span>
                <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="n">window_len</span><span class="p">]</span>
                    <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="n">num_tokens_to_remove</span><span class="p">:]</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
                    <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;invalid truncation strategy: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="si">}</span><span class="s2">, use &#39;left&#39; or &#39;right&#39;.&quot;</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="n">error_msg</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;We need to remove </span><span class="si">{</span><span class="n">num_tokens_to_remove</span><span class="si">}</span><span class="s2"> to truncate the input &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but the first sequence has a length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">ONLY_FIRST</span><span class="p">:</span>
                    <span class="n">error_msg</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">error_msg</span>
                        <span class="o">+</span> <span class="s2">&quot;Please select another truncation strategy than &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">truncation_strategy</span><span class="si">}</span><span class="s2">, for instance &#39;longest_first&#39; or &#39;only_second&#39;.&quot;</span>
                    <span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="n">error_msg</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Be aware, overflowing tokens are not returned for the setting you have chosen,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; i.e. sequence pairs with the &#39;</span><span class="si">{</span><span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">LONGEST_FIRST</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2">&#39; &quot;</span>
                <span class="s2">&quot;truncation strategy. So the returned list will always be empty even if some &quot;</span>
                <span class="s2">&quot;tokens have been removed.&quot;</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tokens_to_remove</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">):</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                        <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                        <span class="n">ids</span> <span class="o">=</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid truncation strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                        <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                        <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid truncation strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">truncation_strategy</span> <span class="o">==</span> <span class="n">TruncationStrategy</span><span class="o">.</span><span class="n">ONLY_SECOND</span> <span class="ow">and</span> <span class="n">pair_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">num_tokens_to_remove</span><span class="p">:</span>
                <span class="n">window_len</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">),</span> <span class="n">stride</span> <span class="o">+</span> <span class="n">num_tokens_to_remove</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[</span><span class="o">-</span><span class="n">window_len</span><span class="p">:]</span>
                    <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="o">-</span><span class="n">num_tokens_to_remove</span><span class="p">]</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                    <span class="n">overflowing_tokens</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[:</span><span class="n">window_len</span><span class="p">]</span>
                    <span class="n">pair_ids</span> <span class="o">=</span> <span class="n">pair_ids</span><span class="p">[</span><span class="n">num_tokens_to_remove</span><span class="p">:]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid truncation strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">truncation_side</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;We need to remove </span><span class="si">{</span><span class="n">num_tokens_to_remove</span><span class="si">}</span><span class="s2"> to truncate the input &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;but the second sequence has a length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pair_ids</span><span class="p">)</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Please select another truncation strategy than </span><span class="si">{</span><span class="n">truncation_strategy</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="s2">&quot;for instance &#39;longest_first&#39; or &#39;only_first&#39;.&quot;</span>
                <span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">pair_ids</span><span class="p">,</span> <span class="n">overflowing_tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_pad</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoded_inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EncodedInput</span><span class="p">],</span> <span class="n">BatchEncoding</span><span class="p">],</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_strategy</span><span class="p">:</span> <span class="n">PaddingStrategy</span> <span class="o">=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span><span class="p">,</span>
        <span class="n">pad_to_multiple_of</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)</span>

<span class="sd">        Args:</span>
<span class="sd">            encoded_inputs:</span>
<span class="sd">                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).</span>
<span class="sd">            max_length: maximum length of the returned list and optionally padding length (see below).</span>
<span class="sd">                Will truncate by taking into account the special tokens.</span>
<span class="sd">            padding_strategy: PaddingStrategy to use for padding.</span>

<span class="sd">                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch</span>
<span class="sd">                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)</span>
<span class="sd">                - PaddingStrategy.DO_NOT_PAD: Do not pad</span>
<span class="sd">                The tokenizer padding sides are defined in self.padding_side:</span>

<span class="sd">                    - &#39;left&#39;: pads on the left of the sequences</span>
<span class="sd">                    - &#39;right&#39;: pads on the right of the sequences</span>
<span class="sd">            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.</span>
<span class="sd">                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability</span>
<span class="sd">                &gt;= 7.5 (Volta).</span>
<span class="sd">            return_attention_mask:</span>
<span class="sd">                (optional) Set to False to avoid returning attention mask (default: set to model specifics)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Load from model defaults</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span>

        <span class="n">required_input</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="k">if</span> <span class="n">padding_strategy</span> <span class="o">==</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">LONGEST</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">pad_to_multiple_of</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">%</span> <span class="n">pad_to_multiple_of</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="p">((</span><span class="n">max_length</span> <span class="o">//</span> <span class="n">pad_to_multiple_of</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">pad_to_multiple_of</span>

        <span class="n">needs_to_be_padded</span> <span class="o">=</span> <span class="n">padding_strategy</span> <span class="o">!=</span> <span class="n">PaddingStrategy</span><span class="o">.</span><span class="n">DO_NOT_PAD</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span> <span class="o">!=</span> <span class="n">max_length</span>

        <span class="c1"># Initialize attention mask if not present.</span>
        <span class="k">if</span> <span class="n">return_attention_mask</span> <span class="ow">and</span> <span class="s2">&quot;attention_mask&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
            <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">needs_to_be_padded</span><span class="p">:</span>
            <span class="n">difference</span> <span class="o">=</span> <span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">required_input</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;right&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>

                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;special_tokens_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">required_input</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">==</span> <span class="s2">&quot;left&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">return_attention_mask</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="s2">&quot;token_type_ids&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_type_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span>
                        <span class="s2">&quot;token_type_ids&quot;</span>
                    <span class="p">]</span>
                <span class="k">if</span> <span class="s2">&quot;special_tokens_mask&quot;</span> <span class="ow">in</span> <span class="n">encoded_inputs</span><span class="p">:</span>
                    <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">encoded_inputs</span><span class="p">[</span><span class="s2">&quot;special_tokens_mask&quot;</span><span class="p">]</span>
                <span class="n">encoded_inputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">model_input_names</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="n">difference</span> <span class="o">+</span> <span class="n">required_input</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid padding strategy:&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">encoded_inputs</span>

    <span class="k">def</span> <span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a sequence of tokens in a single string. The most simple way to do it is `&quot; &quot;.join(tokens)` but we</span>
<span class="sd">        often want to remove sub-word tokenization artifacts at the same time.</span>

<span class="sd">        Args:</span>
<span class="sd">            tokens (`List[str]`): The token to join in a string.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `str`: The joined tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">batch_decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sequences</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="s2">&quot;np.ndarray&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a list of lists of token ids into a list of strings by calling decode.</span>

<span class="sd">        Args:</span>
<span class="sd">            sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):</span>
<span class="sd">                List of tokenized input ids. Can be obtained using the `__call__` method.</span>
<span class="sd">            skip_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to remove special tokens in the decoding.</span>
<span class="sd">            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to clean up the tokenization spaces.</span>
<span class="sd">            kwargs (additional keyword arguments, *optional*):</span>
<span class="sd">                Will be passed to the underlying model specific decode method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `List[str]`: The list of decoded sentences.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                <span class="n">seq</span><span class="p">,</span>
                <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">,</span>
                <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">clean_up_tokenization_spaces</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">seq</span> <span class="ow">in</span> <span class="n">sequences</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">token_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;np.ndarray&quot;</span><span class="p">,</span> <span class="s2">&quot;torch.Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;tf.Tensor&quot;</span><span class="p">],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special</span>
<span class="sd">        tokens and clean up tokenization spaces.</span>

<span class="sd">        Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):</span>
<span class="sd">                List of tokenized input ids. Can be obtained using the `__call__` method.</span>
<span class="sd">            skip_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not to remove special tokens in the decoding.</span>
<span class="sd">            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):</span>
<span class="sd">                Whether or not to clean up the tokenization spaces.</span>
<span class="sd">            kwargs (additional keyword arguments, *optional*):</span>
<span class="sd">                Will be passed to the underlying model specific decode method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `str`: The decoded sentence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert inputs to python lists</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">to_py_obj</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decode</span><span class="p">(</span>
            <span class="n">token_ids</span><span class="o">=</span><span class="n">token_ids</span><span class="p">,</span>
            <span class="n">skip_special_tokens</span><span class="o">=</span><span class="n">skip_special_tokens</span><span class="p">,</span>
            <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="n">clean_up_tokenization_spaces</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">token_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">skip_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">clean_up_tokenization_spaces</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">get_special_tokens_mask</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">token_ids_0</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">token_ids_1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">already_has_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding</span>
<span class="sd">        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.</span>

<span class="sd">        Args:</span>
<span class="sd">            token_ids_0 (`List[int]`):</span>
<span class="sd">                List of ids of the first sequence.</span>
<span class="sd">            token_ids_1 (`List[int]`, *optional*):</span>
<span class="sd">                List of ids of the second sequence.</span>
<span class="sd">            already_has_special_tokens (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether or not the token list is already formatted with special tokens for the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">already_has_special_tokens</span> <span class="ow">and</span> <span class="n">token_ids_1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="p">(</span>
            <span class="s2">&quot;You cannot use ``already_has_special_tokens=False`` with this tokenizer. &quot;</span>
            <span class="s2">&quot;Please use a slow (full python) tokenizer to activate this argument. &quot;</span>
            <span class="s2">&quot;Or set `return_special_tokens_mask=True` when calling the encoding method &quot;</span>
            <span class="s2">&quot;to get the special tokens mask in any tokenizer. &quot;</span>
        <span class="p">)</span>

        <span class="n">all_special_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_special_ids</span>  <span class="c1"># cache the property</span>

        <span class="n">special_tokens_mask</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">all_special_ids</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_ids_0</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">special_tokens_mask</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">clean_up_tokenization</span><span class="p">(</span><span class="n">out_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.</span>

<span class="sd">        Args:</span>
<span class="sd">            out_string (`str`): The text to clean up.</span>

<span class="sd">        Returns:</span>
<span class="sd">            `str`: The cleaned-up string.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">out_string</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">out_string</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; .&quot;</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ?&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; !&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; ,&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39; &quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; n&#39;t&quot;</span><span class="p">,</span> <span class="s2">&quot;n&#39;t&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;m&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;m&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;s&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;ve&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;ve&quot;</span><span class="p">)</span>
            <span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &#39;re&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;re&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">out_string</span>

    <span class="k">def</span> <span class="nf">_eventual_warn_about_too_long_sequence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Depending on the input and internal state we might trigger a warning about a sequence that is too long for its</span>
<span class="sd">        corresponding model</span>

<span class="sd">        Args:</span>
<span class="sd">            ids (`List[str]`): The ids produced by the tokenization</span>
<span class="sd">            max_length (`int`, *optional*): The max_length desired (does not trigger a warning if it is set)</span>
<span class="sd">            verbose (`bool`): Whether or not to print more information and warnings.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span> <span class="ow">and</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sequence-length-is-longer-than-the-specified-maximum&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Token indices sequence length is longer than the specified maximum sequence length &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;for this model (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span><span class="si">}</span><span class="s2"> &gt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span><span class="si">}</span><span class="s2">). Running this sequence through the model &quot;</span>
                    <span class="s2">&quot;will result in indexing errors&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">deprecation_warnings</span><span class="p">[</span><span class="s2">&quot;sequence-length-is-longer-than-the-specified-maximum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_switch_to_input_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method to put the tokenizer in input mode (when it has different modes for input/outputs)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_switch_to_target_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Private method to put the tokenizer in target mode (when it has different modes for input/outputs)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@contextmanager</span>
    <span class="k">def</span> <span class="nf">as_target_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to</span>
<span class="sd">        sequence-to-sequence models that need a slightly different processing for the labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your &quot;</span>
            <span class="s2">&quot;labels by using the argument `text_target` of the regular `__call__` method (either in the same call as &quot;</span>
            <span class="s2">&quot;your input texts if you use the same keyword arguments, or in a separate call.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_target_mode</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_target_context_manager</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">yield</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_in_target_context_manager</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_switch_to_input_mode</span><span class="p">()</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">register_for_auto_class</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">auto_class</span><span class="o">=</span><span class="s2">&quot;AutoTokenizer&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the</span>
<span class="sd">        library are already mapped with `AutoTokenizer`.</span>

<span class="sd">        &lt;Tip warning={true}&gt;</span>

<span class="sd">        This API is experimental and may have some slight breaking changes in the next releases.</span>

<span class="sd">        &lt;/Tip&gt;</span>

<span class="sd">        Args:</span>
<span class="sd">            auto_class (`str` or `type`, *optional*, defaults to `&quot;AutoTokenizer&quot;`):</span>
<span class="sd">                The auto class to register this new tokenizer with.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_class</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">auto_class</span> <span class="o">=</span> <span class="n">auto_class</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="kn">import</span> <span class="nn">transformers.models.auto</span> <span class="k">as</span> <span class="nn">auto_module</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">auto_module</span><span class="p">,</span> <span class="n">auto_class</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">auto_class</span><span class="si">}</span><span class="s2"> is not a valid auto class.&quot;</span><span class="p">)</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">_auto_class</span> <span class="o">=</span> <span class="n">auto_class</span>

    <span class="k">def</span> <span class="nf">prepare_seq2seq_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">src_texts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">tgt_texts</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_target_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;longest&quot;</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BatchEncoding</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare model inputs for translation. For best performance, translate one sentence at a time.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            src_texts (`List[str]`):</span>
<span class="sd">                List of documents to summarize or source language texts.</span>
<span class="sd">            tgt_texts (`list`, *optional*):</span>
<span class="sd">                List of summaries or target language texts.</span>
<span class="sd">            max_length (`int`, *optional*):</span>
<span class="sd">                Controls the maximum length for encoder inputs (documents to summarize or source language texts) If</span>
<span class="sd">                left unset or set to `None`, this will use the predefined model maximum length if a maximum length is</span>
<span class="sd">                required by one of the truncation/padding parameters. If the model has no specific maximum input length</span>
<span class="sd">                (like XLNet) truncation/padding to a maximum length will be deactivated.</span>
<span class="sd">            max_target_length (`int`, *optional*):</span>
<span class="sd">                Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set</span>
<span class="sd">                to `None`, this will use the max_length value.</span>
<span class="sd">            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):</span>
<span class="sd">                Activates and controls padding. Accepts the following values:</span>

<span class="sd">                - `True` or `&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single</span>
<span class="sd">                  sequence if provided).</span>
<span class="sd">                - `&#39;max_length&#39;`: Pad to a maximum length specified with the argument `max_length` or to the maximum</span>
<span class="sd">                  acceptable input length for the model if that argument is not provided.</span>
<span class="sd">                - `False` or `&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different</span>
<span class="sd">                  lengths).</span>
<span class="sd">            return_tensors (`str` or [`~utils.TensorType`], *optional*):</span>
<span class="sd">                If set, will return tensors instead of list of python integers. Acceptable values are:</span>

<span class="sd">                - `&#39;tf&#39;`: Return TensorFlow `tf.constant` objects.</span>
<span class="sd">                - `&#39;pt&#39;`: Return PyTorch `torch.Tensor` objects.</span>
<span class="sd">                - `&#39;np&#39;`: Return Numpy `np.ndarray` objects.</span>
<span class="sd">            truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):</span>
<span class="sd">                Activates and controls truncation. Accepts the following values:</span>

<span class="sd">                - `True` or `&#39;longest_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or</span>
<span class="sd">                  to the maximum acceptable input length for the model if that argument is not provided. This will</span>
<span class="sd">                  truncate token by token, removing a token from the longest sequence in the pair if a pair of</span>
<span class="sd">                  sequences (or a batch of pairs) is provided.</span>
<span class="sd">                - `&#39;only_first&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                - `&#39;only_second&#39;`: Truncate to a maximum length specified with the argument `max_length` or to the</span>
<span class="sd">                  maximum acceptable input length for the model if that argument is not provided. This will only</span>
<span class="sd">                  truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</span>
<span class="sd">                - `False` or `&#39;do_not_truncate&#39;` (default): No truncation (i.e., can output batch with sequence lengths</span>
<span class="sd">                  greater than the model maximum admissible input size).</span>
<span class="sd">            **kwargs:</span>
<span class="sd">                Additional keyword arguments passed along to `self.__call__`.</span>

<span class="sd">        Return:</span>
<span class="sd">            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:</span>

<span class="sd">            - **input_ids** -- List of token ids to be fed to the encoder.</span>
<span class="sd">            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.</span>
<span class="sd">            - **labels** -- List of token ids for tgt_texts.</span>

<span class="sd">            The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.</span>
<span class="sd">            Otherwise, input_ids, attention_mask will be the only keys.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># docstyle-ignore</span>
        <span class="n">formatted_warning</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular</span>
<span class="s2">`__call__` method to prepare your inputs and targets.</span>

<span class="s2">Here is a short example:</span>

<span class="s2">model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)</span>

<span class="s2">If you either need to use different keyword arguments for the source and target texts, you should do two calls like</span>
<span class="s2">this:</span>

<span class="s2">model_inputs = tokenizer(src_texts, ...)</span>
<span class="s2">labels = tokenizer(text_target=tgt_texts, ...)</span>
<span class="s2">model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]</span>

<span class="s2">See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.</span>
<span class="s2">For a more complete example, see the implementation of `prepare_seq2seq_batch`.</span>
<span class="s2">&quot;&quot;&quot;</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">formatted_warning</span><span class="p">,</span> <span class="ne">FutureWarning</span><span class="p">)</span>
        <span class="c1"># mBART-specific kwargs that should be ignored by other models.</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;src_lang&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tgt_lang&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_length</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
            <span class="n">src_texts</span><span class="p">,</span>
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">tgt_texts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_inputs</span>
        <span class="c1"># Process tgt_texts</span>
        <span class="k">if</span> <span class="n">max_target_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">max_target_length</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">as_target_tokenizer</span><span class="p">():</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="n">tgt_texts</span><span class="p">,</span>
                <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">max_target_length</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">model_inputs</span>


<span class="k">def</span> <span class="nf">get_fast_tokenizer_file</span><span class="p">(</span><span class="n">tokenization_files</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the tokenization file to use for this version of transformers.</span>

<span class="sd">    Args:</span>
<span class="sd">        tokenization_files (`List[str]`): The list of available configuration files.</span>

<span class="sd">    Returns:</span>
<span class="sd">        `str`: The tokenization file to use.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tokenizer_files_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">file_name</span> <span class="ow">in</span> <span class="n">tokenization_files</span><span class="p">:</span>
        <span class="n">search</span> <span class="o">=</span> <span class="n">_re_tokenizer_file</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">search</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">groups</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">tokenizer_files_map</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_name</span>
    <span class="n">available_versions</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">tokenizer_files_map</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="c1"># Defaults to FULL_TOKENIZER_FILE and then try to look at some newer versions.</span>
    <span class="n">tokenizer_file</span> <span class="o">=</span> <span class="n">FULL_TOKENIZER_FILE</span>
    <span class="n">transformers_version</span> <span class="o">=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">__version__</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">available_versions</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">transformers_version</span><span class="p">:</span>
            <span class="n">tokenizer_file</span> <span class="o">=</span> <span class="n">tokenizer_files_map</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># No point going further since the versions are sorted.</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">tokenizer_file</span>


<span class="c1"># To update the docstring, we need to copy the method, otherwise we change the original docstring.</span>
<span class="n">PreTrainedTokenizerBase</span><span class="o">.</span><span class="n">push_to_hub</span> <span class="o">=</span> <span class="n">copy_func</span><span class="p">(</span><span class="n">PreTrainedTokenizerBase</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">)</span>
<span class="n">PreTrainedTokenizerBase</span><span class="o">.</span><span class="n">push_to_hub</span><span class="o">.</span><span class="vm">__doc__</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerBase</span><span class="o">.</span><span class="n">push_to_hub</span><span class="o">.</span><span class="vm">__doc__</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">object</span><span class="o">=</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span> <span class="n">object_class</span><span class="o">=</span><span class="s2">&quot;AutoTokenizer&quot;</span><span class="p">,</span> <span class="n">object_files</span><span class="o">=</span><span class="s2">&quot;tokenizer files&quot;</span>
<span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, McAuley Lab.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>